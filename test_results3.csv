original_snippet,edited_snippet,project,optimizer,prompt,prompt_type,failed_attempts,avg_runtime
"def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)

        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]



    if ndim <= 2:

        result = result[0, 0]



    return result
","def median_filter(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""
    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), ""`filter_width` should be an odd positive number""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension.
        return x

    # Store original ndim to reshape back at the end.
    ndim = x.ndim
    if ndim < 4:
        # F.pad with 'reflect' mode needs at least 3D input. We promote to 4D
        # for a consistent padding logic that only affects the last dimension.
        x = x.view([1] * (4 - ndim) + list(x.shape))

    # Pad the tensor only along the last dimension of the 4D tensor.
    x = F.pad(x, (pad_width, pad_width, 0, 0), mode=""reflect"")

    result = None
    if x.is_cuda:
        try:
            # Attempt to use a custom, highly optimized Triton kernel for CUDA tensors.
            from .triton_ops import median_filter_cuda

            result = median_filter_cuda(x, filter_width)
        except (ImportError, RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit or installation issue; ""
                ""falling back to a slower median kernel implementation...""
            )

    if result is None:
        # Fallback for CPU or if the Triton kernel fails.
        # .unfold() creates sliding window views of the tensor without data duplication.
        windows = x.unfold(-1, filter_width, 1)
        
        # Use torch.kthvalue to find the median element (k-th smallest).
        # This is algorithmically more efficient (O(N)) than a full sort (O(N log N))
        # for finding a single element in each window.
        # For a window of size `filter_width`, the median is the (pad_width + 1)-th element.
        median_values = torch.kthvalue(windows, k=pad_width + 1, dim=-1).values
        result = median_values

    if ndim < 4:
        # Reshape the result back to its original number of dimensions.
        result = result.view(result.shape[4 - ndim :])

    return result",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,4,12.950700000000001
"def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel



    M, N = x.shape

    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""



    x_skew = (

        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)

    )

    x_skew = x_skew.T.contiguous()

    cost = torch.ones(N + M + 2, M + 2) * np.inf

    cost[0, 0] = 0

    cost = cost.to(x.device)

    trace = torch.zeros_like(cost, dtype=torch.int32)



    dtw_kernel[(1,)](

        cost,

        trace,

        x_skew,

        x_skew.stride(0),

        cost.stride(0),

        trace.stride(0),

        N,

        M,

        BLOCK_SIZE=BLOCK_SIZE,

    )



    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[

        :, : N + 1

    ]

    return backtrace(trace.cpu().numpy())
","def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""

    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
        .T.contiguous()
    )

    cost = torch.full(
        (N + M + 2, M + 2), float('inf'), dtype=x.dtype, device=x.device
    )
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[
        :, : N + 1
    ]
    return backtrace(trace.cpu().numpy())",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"@lru_cache(maxsize=None)

def get_tokenizer(

    multilingual: bool,

    *,

    num_languages: int = 99,

    language: Optional[str] = None,

    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]

) -> Tokenizer:

    if language is not None:

        language = language.lower()

        if language not in LANGUAGES:

            if language in TO_LANGUAGE_CODE:

                language = TO_LANGUAGE_CODE[language]

            else:

                raise ValueError(f""Unsupported language: {language}"")



    if multilingual:

        encoding_name = ""multilingual""

        language = language or ""en""

        task = task or ""transcribe""

    else:

        encoding_name = ""gpt2""

        language = None

        task = None



    encoding = get_encoding(name=encoding_name, num_languages=num_languages)



    return Tokenizer(

        encoding=encoding, num_languages=num_languages, language=language, task=task

    )
","@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]
) -> Tokenizer:
    if multilingual:
        encoding_name = ""multilingual""
        if language is not None:
            lang_lower = language.lower()
            if lang_lower not in LANGUAGES:
                language = TO_LANGUAGE_CODE.get(lang_lower)
                if language is None:
                    raise ValueError(f""Unsupported language: {lang_lower}"")
            else:
                language = lang_lower
        
        language = language or ""en""
        task = task or ""transcribe""
    else:
        encoding_name = ""gpt2""
        language = None
        task = None

    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model.to(device)
","def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims).to(device)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,1,12.950700000000001
"def encode(self, text, **kwargs):

    return self.encoding.encode(text, **kwargs)
","def encode(self, text, **kwargs):
    return self.encoding.encode(text, **kwargs)",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def __init__(self, dims: ModelDimensions):

    super().__init__()

    self.dims = dims

    self.encoder = AudioEncoder(

        self.dims.n_mels,

        self.dims.n_audio_ctx,

        self.dims.n_audio_state,

        self.dims.n_audio_head,

        self.dims.n_audio_layer,

    )

    self.decoder = TextDecoder(

        self.dims.n_vocab,

        self.dims.n_text_ctx,

        self.dims.n_text_state,

        self.dims.n_text_head,

        self.dims.n_text_layer,

    )

    # use the last half among the decoder layers for time alignment by default;

    # to use a specific set of heads, see `set_alignment_heads()` below.

    all_heads = torch.zeros(

        self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool

    )

    all_heads[self.dims.n_text_layer // 2 :] = True

    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)
","def __init__(self, dims: ModelDimensions):
    super().__init__()
    self.dims = dims
    self.encoder = AudioEncoder(
        self.dims.n_mels,
        self.dims.n_audio_ctx,
        self.dims.n_audio_state,
        self.dims.n_audio_head,
        self.dims.n_audio_layer,
    )
    self.decoder = TextDecoder(
        self.dims.n_vocab,
        self.dims.n_text_ctx,
        self.dims.n_text_state,
        self.dims.n_text_head,
        self.dims.n_text_layer,
    )

    # Optimized sparse tensor creation for alignment heads.
    # This approach directly constructs the sparse tensor from its indices and values,
    # avoiding the allocation of a large intermediate dense tensor.
    # This is more memory-efficient and reduces initialization time, especially for large models.
    start_layer = self.dims.n_text_layer // 2
    layer_indices, head_indices = torch.meshgrid(
        torch.arange(start_layer, self.dims.n_text_layer),
        torch.arange(self.dims.n_text_head),
        indexing=""ij"",
    )

    indices = torch.stack([layer_indices.flatten(), head_indices.flatten()])
    values = torch.ones(indices.shape[1], dtype=torch.bool)
    size = (self.dims.n_text_layer, self.dims.n_text_head)

    alignment_heads = torch.sparse_coo_tensor(indices, values, size)
    self.register_buffer(""alignment_heads"", alignment_heads, persistent=False)",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of given width along the last dimension of x""""""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y
","def process_audio_data(audio_chunks):
    return np.log(np.abs(np.fft.fft(audio_chunks, axis=-1)) + 1e-10)",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"@lru_cache(maxsize=None)

def get_encoding(name: str = ""gpt2"", num_languages: int = 99):

    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")

    ranks = {

        base64.b64decode(token): int(rank)

        for token, rank in (line.split() for line in open(vocab_path) if line)

    }

    n_vocab = len(ranks)

    special_tokens = {}



    specials = [

        ""<|endoftext|>"",

        ""<|startoftranscript|>"",

        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],

        ""<|translate|>"",

        ""<|transcribe|>"",

        ""<|startoflm|>"",

        ""<|startofprev|>"",

        ""<|nospeech|>"",

        ""<|notimestamps|>"",

        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],

    ]



    for token in specials:

        special_tokens[token] = n_vocab

        n_vocab += 1



    return tiktoken.Encoding(

        name=os.path.basename(vocab_path),

        explicit_n_vocab=n_vocab,

        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",

        mergeable_ranks=ranks,

        special_tokens=special_tokens,

    )
","@lru_cache(maxsize=None)
def get_encoding(name: str = ""gpt2"", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")
    # Using a 'with' statement provides robust, exception-safe file handling and
    # explicit UTF-8 encoding is a best practice. The generator expression for
    # splitting lines is memory-efficient, avoiding loading the entire file content
    # into an intermediate list.
    with open(vocab_path, ""r"", encoding=""utf-8"") as f:
        ranks = {
            base64.b64decode(token): int(rank)
            for token, rank in (line.split() for line in f if line.strip())
        }

    n_vocab = len(ranks)

    # The list of special tokens is constructed. The original construction using
    # list unpacking is retained as it is both concise and performant for this task.
    specials = [
        ""<|endoftext|>"",
        ""<|startoftranscript|>"",
        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],
        ""<|translate|>"",
        ""<|transcribe|>"",
        ""<|startoflm|>"",
        ""<|startofprev|>"",
        ""<|nospeech|>"",
        ""<|notimestamps|>"",
        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],
    ]

    # A dictionary comprehension with enumerate is substituted for the original for-loop.
    # This approach is generally faster as the looping mechanism is implemented in C,
    # reducing Python interpreter overhead.
    special_tokens = {token: n_vocab + i for i, token in enumerate(specials)}

    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab + len(specials),
        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
","def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")

    if os.path.isfile(download_target):
        # Calculate hash of existing file in chunks to avoid loading it all into memory.
        hasher = hashlib.sha256()
        with open(download_target, ""rb"") as f:
            # Use walrus operator for cleaner chunk reading
            while chunk := f.read(8192):
                hasher.update(chunk)

        if hasher.hexdigest() == expected_sha256:
            if in_memory:
                # The file is valid and in-memory return is requested. Read it once.
                with open(download_target, ""rb"") as f:
                    return f.read()
            else:
                return download_target
        else:
            warnings.warn(
                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
            )

    # --- Download and verify simultaneously --- #
    hasher = hashlib.sha256()
    # Use bytearray for efficient in-memory buffer accumulation
    model_buffer = bytearray() if in_memory else None

    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:
        # Handle cases where Content-Length header is not present
        content_length = source.info().get(""Content-Length"")
        total = int(content_length) if content_length else None

        with tqdm(
            total=total,
            ncols=80,
            unit=""iB"",
            unit_scale=True,
            unit_divisor=1024,
        ) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                # Perform all operations in a single pass over the chunk
                output.write(buffer)
                hasher.update(buffer)
                if model_buffer is not None:
                    model_buffer.extend(buffer)
                loop.update(len(buffer))

    # Final verification after download
    if hasher.hexdigest() != expected_sha256:
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    return bytes(model_buffer) if in_memory else download_target",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def load_audio(file: str, sr: int = SAMPLE_RATE):

    """"""

    Open an audio file and read as mono waveform, resampling as necessary



    Parameters

    ----------

    file: str

        The audio file to open



    sr: int

        The sample rate to resample the audio if necessary



    Returns

    -------

    A NumPy array containing the audio waveform, in float32 dtype.

    """"""



    # This launches a subprocess to decode audio while down-mixing

    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.

    # fmt: off

    cmd = [

        ""ffmpeg"",

        ""-nostdin"",

        ""-threads"", ""0"",

        ""-i"", file,

        ""-f"", ""s16le"",

        ""-ac"", ""1"",

        ""-acodec"", ""pcm_s16le"",

        ""-ar"", str(sr),

        ""-""

    ]

    # fmt: on

    try:

        out = run(cmd, capture_output=True, check=True).stdout

    except CalledProcessError as e:

        raise RuntimeError(f""Failed to load audio: {e.stderr.decode()}"") from e



    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0
","def load_audio(file: str, sr: int = SAMPLE_RATE):
    """"""
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """"""
    try:
        # Use PyAV for in-process FFMPEG access, avoiding subprocess overhead.
        with av.open(file, mode=""r"") as container:
            # Select the first audio stream, enabling multi-threaded decoding.
            stream = container.streams.audio[0]
            stream.thread_type = ""AUTO""

            # Create a resampler to convert audio to the target format (s16 mono).
            resampler = av.AudioResampler(
                format=""s16"",
                layout=""mono"",
                rate=sr,
            )

            # Decode, resample, and write raw bytes to an efficient in-memory buffer.
            buffer = io.BytesIO()
            for frame in container.decode(stream):
                for resampled_frame in resampler.resample(frame):
                    buffer.write(resampled_frame.planes[0].to_bytes())

            raw_audio = buffer.getvalue()
            if not raw_audio:
                return np.empty(0, dtype=np.float32)

            # Interpret the byte buffer as a NumPy array of 16-bit integers.
            audio_np = np.frombuffer(raw_audio, dtype=np.int16)
            
            # Convert to float32 and normalize to the [-1.0, 1.0) range.
            return audio_np.astype(np.float32) / 32768.0

    except av.AVError as e:
        raise RuntimeError(f""Failed to load audio with PyAV: {e.stderr.decode()}"") from e
    except IndexError:
        raise RuntimeError(f""File '{file}' contains no audio streams."") from None",whisper,25,"Optimize the following code object for runtime performance, maintaining its exact signature and interface. Apply detailed reasoning and analysis to address algorithmic complexity, data structures, loop optimizations, memory access, I/O operations, and potential for parallel processing. After thorough evaluation and strategy selection, provide only the optimized version of the code object, leaving all surrounding code and parent classes untouched.

```python
def process_audio_data(audio_chunks):
    processed_data = []
    for chunk in audio_chunks:
        fft_result = np.fft.fft(chunk)
        magnitude = np.abs(fft_result)
        log_magnitude = np.log(magnitude + 1e-10)
        processed_data.append(log_magnitude)
    return np.array(processed_data)
```
",MP,0,12.950700000000001
"def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)

        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]



    if ndim <= 2:

        result = result[0, 0]



    return result
","def median_filter(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""
    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), ""`filter_width` should be an odd number""

    pad_width = filter_width // 2
    if pad_width == 0:
        return x

    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    ndim = x.ndim
    if ndim <= 2:
        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D
        x = x[None, None, :]

    result = None
    x = F.pad(x, (pad_width, pad_width, 0, 0), mode=""reflect"")

    if x.is_cuda:
        try:
            from .triton_ops import median_filter_cuda

            result = median_filter_cuda(x, filter_width)
        except (RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""
                ""falling back to a slower median kernel implementation...""
            )

    if result is None:
        # unfold creates sliding windows; kthvalue is O(N) to find the median of a window of size N
        # which is more efficient than sort's O(N log N).
        windows = x.unfold(-1, filter_width, 1)
        # .kthvalue's k is 1-indexed, so we use pad_width + 1 for the median
        result = windows.kthvalue(k=pad_width + 1, dim=-1).values

    if ndim <= 2:
        result = result[0, 0]

    return result",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel



    M, N = x.shape

    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""



    x_skew = (

        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)

    )

    x_skew = x_skew.T.contiguous()

    cost = torch.ones(N + M + 2, M + 2) * np.inf

    cost[0, 0] = 0

    cost = cost.to(x.device)

    trace = torch.zeros_like(cost, dtype=torch.int32)



    dtw_kernel[(1,)](

        cost,

        trace,

        x_skew,

        x_skew.stride(0),

        cost.stride(0),

        trace.stride(0),

        N,

        M,

        BLOCK_SIZE=BLOCK_SIZE,

    )



    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[

        :, : N + 1

    ]

    return backtrace(trace.cpu().numpy())
","def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""

    x_skew = F.pad(x, (0, M + 1), value=float('inf'))
    x_skew = x_skew.flatten()[: M * (N + M)].reshape(M, N + M)
    x_skew = x_skew.T.contiguous()

    # Initialize cost and trace matrices directly on the target device to avoid CPU->GPU copies.
    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    # Un-skew the trace matrix to recover the standard alignment path matrix.
    trace = trace.T.flatten()[: (M + 1) * (N + M + 3)].reshape(M + 1, N + M + 3)[
        :, : N + 1
    ]
    
    # Backtracing is a sequential operation, best performed on the CPU.
    return backtrace(trace.cpu().numpy())",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,4,12.6062
"@lru_cache(maxsize=None)

def get_tokenizer(

    multilingual: bool,

    *,

    num_languages: int = 99,

    language: Optional[str] = None,

    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]

) -> Tokenizer:

    if language is not None:

        language = language.lower()

        if language not in LANGUAGES:

            if language in TO_LANGUAGE_CODE:

                language = TO_LANGUAGE_CODE[language]

            else:

                raise ValueError(f""Unsupported language: {language}"")



    if multilingual:

        encoding_name = ""multilingual""

        language = language or ""en""

        task = task or ""transcribe""

    else:

        encoding_name = ""gpt2""

        language = None

        task = None



    encoding = get_encoding(name=encoding_name, num_languages=num_languages)



    return Tokenizer(

        encoding=encoding, num_languages=num_languages, language=language, task=task

    )
","@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,
) -> Tokenizer:
    if not multilingual:
        encoding_name = ""gpt2""
        language = None
        task = None
    else:
        encoding_name = ""multilingual""
        task = task or ""transcribe""
        if language is not None:
            language_lower = language.lower()
            if language_lower not in LANGUAGES:
                language_lower = TO_LANGUAGE_CODE.get(language_lower)
                if language_lower is None:
                    raise ValueError(f""Unsupported language: {language}"")
            language = language_lower
        language = language or ""en""

    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model.to(device)
","def load_model(
    name: str,
    device: Optional[Union[str, torch.device]] = None,
    download_root: str = None,
    in_memory: bool = False,
) -> Whisper:
    """"""
    Load a Whisper ASR model

    Parameters
    ----------
    name : str
        one of the official model names listed by `whisper.available_models()`, or
        path to a model checkpoint containing the model dimensions and the model state_dict.
    device : Union[str, torch.device]
        the PyTorch device to put the model into
    download_root: str
        path to download the model files; by default, it uses ""~/.cache/whisper""
    in_memory: bool
        whether to preload the model weights into host memory

    Returns
    -------
    model : Whisper
        The Whisper ASR model instance
    """"""

    if device is None:
        device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    if download_root is None:
        default = os.path.join(os.path.expanduser(""~""), "".cache"")
        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")

    if name in _MODELS:
        checkpoint_file = _download(_MODELS[name], download_root, in_memory)
        alignment_heads = _ALIGNMENT_HEADS[name]
    elif os.path.isfile(name):
        checkpoint_file = open(name, ""rb"").read() if in_memory else name
        alignment_heads = None
    else:
        raise RuntimeError(
            f""Model {name} not found; available models = {available_models()}""
        )

    with (
        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")
    ) as fp:
        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}
        checkpoint = torch.load(fp, map_location=device, **kwargs)
    del checkpoint_file

    dims = ModelDimensions(**checkpoint[""dims""])
    model = Whisper(dims).to(device)
    model.load_state_dict(checkpoint[""model_state_dict""])

    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)

    return model",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def encode(self, text, **kwargs):

    return self.encoding.encode(text, **kwargs)
","def encode(self, text, **kwargs):
    return self.encoding.encode(text, **kwargs)",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def __init__(self, dims: ModelDimensions):

    super().__init__()

    self.dims = dims

    self.encoder = AudioEncoder(

        self.dims.n_mels,

        self.dims.n_audio_ctx,

        self.dims.n_audio_state,

        self.dims.n_audio_head,

        self.dims.n_audio_layer,

    )

    self.decoder = TextDecoder(

        self.dims.n_vocab,

        self.dims.n_text_ctx,

        self.dims.n_text_state,

        self.dims.n_text_head,

        self.dims.n_text_layer,

    )

    # use the last half among the decoder layers for time alignment by default;

    # to use a specific set of heads, see `set_alignment_heads()` below.

    all_heads = torch.zeros(

        self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool

    )

    all_heads[self.dims.n_text_layer // 2 :] = True

    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)
","def __init__(self, dims: ModelDimensions):
    super().__init__()
    self.dims = dims

    self.encoder = AudioEncoder(
        dims.n_mels,
        dims.n_audio_ctx,
        dims.n_audio_state,
        dims.n_audio_head,
        dims.n_audio_layer,
    )
    self.decoder = TextDecoder(
        dims.n_vocab,
        dims.n_text_ctx,
        dims.n_text_state,
        dims.n_text_head,
        dims.n_text_layer,
    )

    # use the last half among the decoder layers for time alignment by default;
    # to use a specific set of heads, see `set_alignment_heads()` below.
    n_layers, n_heads = dims.n_text_layer, dims.n_text_head
    layer_mask = torch.arange(n_layers) >= n_layers // 2
    all_heads = layer_mask.unsqueeze(1).expand(-1, n_heads)
    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of given width along the last dimension of x""""""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y
","def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of given width along the last dimension of x""""""
    # Create a view of the tensor with sliding windows and compute the median.
    # This is significantly more efficient than a custom CUDA kernel launch
    # as it leverages PyTorch's highly optimized, pre-compiled backend.
    slices = x.unfold(-1, filter_width, 1)
    return slices.median(dim=-1).values",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,1,12.6062
"@lru_cache(maxsize=None)

def get_encoding(name: str = ""gpt2"", num_languages: int = 99):

    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")

    ranks = {

        base64.b64decode(token): int(rank)

        for token, rank in (line.split() for line in open(vocab_path) if line)

    }

    n_vocab = len(ranks)

    special_tokens = {}



    specials = [

        ""<|endoftext|>"",

        ""<|startoftranscript|>"",

        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],

        ""<|translate|>"",

        ""<|transcribe|>"",

        ""<|startoflm|>"",

        ""<|startofprev|>"",

        ""<|nospeech|>"",

        ""<|notimestamps|>"",

        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],

    ]



    for token in specials:

        special_tokens[token] = n_vocab

        n_vocab += 1



    return tiktoken.Encoding(

        name=os.path.basename(vocab_path),

        explicit_n_vocab=n_vocab,

        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",

        mergeable_ranks=ranks,

        special_tokens=special_tokens,

    )
","# Module-level constants and cached helper function for performance.
# Caches the expensive file reading and parsing operation.
@lru_cache(maxsize=None)
def _load_ranks(vocab_path: str):
    with open(vocab_path, ""r"", encoding=""utf-8"") as f:
        lines = (line.split() for line in f if line)
        return {base64.b64decode(token): int(rank) for token, rank in lines}

# Pre-computing constant lists avoids regeneration on every function call.
_LANGUAGE_KEYS = list(LANGUAGES.keys())
_PREFIX_SPECIALS = [""<|endoftext|>"", ""<|startoftranscript|>""]
_SUFFIX_SPECIALS = (
    [
        ""<|translate|>"",
        ""<|transcribe|>"",
        ""<|startoflm|>"",
        ""<|startofprev|>"",
        ""<|nospeech|>"",
        ""<|notimestamps|>"",
    ]
    + [f""<|{i * 0.02:.2f}|>"" for i in range(1501)]
)

@lru_cache(maxsize=None)
def get_encoding(name: str = ""gpt2"", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")
    ranks = _load_ranks(vocab_path)

    n_vocab = len(ranks)

    # Dynamically generate language-specific tokens and combine with pre-computed constants.
    language_specials = [f""<|{lang}|>"" for lang in _LANGUAGE_KEYS[:num_languages]]
    specials = _PREFIX_SPECIALS + language_specials + _SUFFIX_SPECIALS

    # Use a dictionary comprehension for efficient creation of the special_tokens map.
    special_tokens = {token: i for i, token in enumerate(specials, start=n_vocab)}

    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab + len(specials),
        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
","def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))

    if os.path.exists(download_target):
        if not os.path.isfile(download_target):
            raise RuntimeError(f""{download_target} exists and is not a regular file"")

        # Optimize: Hash the existing file in chunks to avoid loading it all into memory.
        sha256 = hashlib.sha256()
        with open(download_target, ""rb"") as f:
            while True:
                buffer = f.read(8192)
                if not buffer:
                    break
                sha256.update(buffer)
        
        if sha256.hexdigest() == expected_sha256:
            if in_memory:
                with open(download_target, ""rb"") as f:
                    return f.read()
            else:
                return download_target
        else:
            warnings.warn(
                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
            )

    # Optimize: Download and compute hash simultaneously to avoid a second file read.
    sha256_download = hashlib.sha256()
    try:
        with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:
            total_size = int(source.info().get(""Content-Length"", 0))
            with tqdm(
                total=total_size if total_size > 0 else None,
                ncols=80,
                unit=""iB"",
                unit_scale=True,
                unit_divisor=1024,
            ) as loop:
                while True:
                    buffer = source.read(8192)
                    if not buffer:
                        break
                    output.write(buffer)
                    sha256_download.update(buffer)
                    loop.update(len(buffer))
    except (urllib.error.URLError, IOError) as e:
        if os.path.exists(download_target):
            os.remove(download_target)
        raise RuntimeError(f""Failed to download {url}: {e}"")

    if sha256_download.hexdigest() != expected_sha256:
        os.remove(download_target)  # Clean up corrupted file
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    if in_memory:
        with open(download_target, ""rb"") as f:
            return f.read()
    else:
        return download_target",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,0,12.6062
"def load_audio(file: str, sr: int = SAMPLE_RATE):

    """"""

    Open an audio file and read as mono waveform, resampling as necessary



    Parameters

    ----------

    file: str

        The audio file to open



    sr: int

        The sample rate to resample the audio if necessary



    Returns

    -------

    A NumPy array containing the audio waveform, in float32 dtype.

    """"""



    # This launches a subprocess to decode audio while down-mixing

    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.

    # fmt: off

    cmd = [

        ""ffmpeg"",

        ""-nostdin"",

        ""-threads"", ""0"",

        ""-i"", file,

        ""-f"", ""s16le"",

        ""-ac"", ""1"",

        ""-acodec"", ""pcm_s16le"",

        ""-ar"", str(sr),

        ""-""

    ]

    # fmt: on

    try:

        out = run(cmd, capture_output=True, check=True).stdout

    except CalledProcessError as e:

        raise RuntimeError(f""Failed to load audio: {e.stderr.decode()}"") from e



    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0
","def load_audio(file: str, sr: int = SAMPLE_RATE):
    """"""
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """"""
    try:
        # librosa.load is a high-level function that is significantly more
        # efficient than shelling out to ffmpeg, as it avoids the overhead
        # of creating a new process for each audio file.
        wav, _ = librosa.load(file, sr=sr, mono=True)
        return wav
    except Exception as e:
        raise RuntimeError(f""Failed to load audio: {e}"") from e",whisper,25,"Here are examples of code optimization:
Example 1 - Loop optimization:
Original: for i in range(len(arr)): if arr[i] > threshold: result.append(arr[i])
Optimized: result = [x for x in arr if x > threshold]

Example 2 - Algorithm optimization:
Original: for i in range(n): for j in range(n): if matrix[i][j] > 0: count += 1
Optimized: count = np.sum(matrix > 0)

Example 3 - Data structure optimization:
Original: items = []; for x in data: items.append(x); return sorted(items)
Optimized: return sorted(data)

Now optimize the code for better runtime performance, then provide only the final optimized code.",FS,5,12.6062
"def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)

        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]



    if ndim <= 2:

        result = result[0, 0]



    return result
","def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # unfold creates sliding windows; median() is more efficient than a full sort
        # as it uses a selection algorithm (O(k)) vs. sort's O(k log k) for window size k.
        windows = x.unfold(-1, filter_width, 1)
        result = windows.median(dim=-1).values



    if ndim <= 2:

        result = result[0, 0]



    return result",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel



    M, N = x.shape

    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""



    x_skew = (

        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)

    )

    x_skew = x_skew.T.contiguous()

    cost = torch.ones(N + M + 2, M + 2) * np.inf

    cost[0, 0] = 0

    cost = cost.to(x.device)

    trace = torch.zeros_like(cost, dtype=torch.int32)



    dtw_kernel[(1,)](

        cost,

        trace,

        x_skew,

        x_skew.stride(0),

        cost.stride(0),

        trace.stride(0),

        N,

        M,

        BLOCK_SIZE=BLOCK_SIZE,

    )



    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[

        :, : N + 1

    ]

    return backtrace(trace.cpu().numpy())
","def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    import torch
    import numpy as np

    M, N = x.shape
    assert M < BLOCK_SIZE, f""M must be smaller than BLOCK_SIZE={BLOCK_SIZE}""

    # 1. Create tensors directly on the target device to avoid H->D transfers.
    # Use torch.inf for clarity and to avoid numpy dependency.
    cost = torch.full((N + M + 2, M + 2), torch.inf, device=x.device, dtype=x.dtype)
    cost[0, 0] = 0.0
    trace = torch.zeros((N + M + 2, M + 2), device=x.device, dtype=torch.int32)

    # 2. Simplified and clearer creation of the skewed input matrix 'x_skew'.
    # This uses advanced indexing to place elements of x onto the anti-diagonals
    # of a larger matrix, which is the goal of the skewing transformation.
    x_skew = torch.full((N + M, M), torch.inf, device=x.device, dtype=x.dtype)
    rows = torch.arange(M, device=x.device).view(M, 1)
    cols = torch.arange(N, device=x.device).view(1, N)
    x_skew[rows + cols, rows] = x

    # Launch the Triton kernel (unchanged)
    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    # 3. Simplified and direct un-skewing of the trace matrix.
    # Advanced indexing is more readable and efficient than the original's
    # complex chain of transpose, flatten, reshape, and slice operations.
    ii, jj = torch.meshgrid(
        torch.arange(M + 1, device=x.device),
        torch.arange(N + 1, device=x.device),
        indexing=""ij"",
    )
    # The un-skewing logic maps standard coordinates (i, j) to skewed (i+j, i)
    unskewed_trace = trace[ii + jj, ii]

    # Transfer the final result to CPU for backtracing (logic unchanged)
    return backtrace(unskewed_trace.cpu().numpy())",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"@lru_cache(maxsize=None)

def get_tokenizer(

    multilingual: bool,

    *,

    num_languages: int = 99,

    language: Optional[str] = None,

    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]

) -> Tokenizer:

    if language is not None:

        language = language.lower()

        if language not in LANGUAGES:

            if language in TO_LANGUAGE_CODE:

                language = TO_LANGUAGE_CODE[language]

            else:

                raise ValueError(f""Unsupported language: {language}"")



    if multilingual:

        encoding_name = ""multilingual""

        language = language or ""en""

        task = task or ""transcribe""

    else:

        encoding_name = ""gpt2""

        language = None

        task = None



    encoding = get_encoding(name=encoding_name, num_languages=num_languages)



    return Tokenizer(

        encoding=encoding, num_languages=num_languages, language=language, task=task

    )
","from functools import lru_cache
from typing import Optional


@lru_cache(maxsize=None)
def _get_tokenizer_cached(
    encoding_name: str,
    num_languages: int,
    language: Optional[str],
    task: Optional[str],
) -> Tokenizer:
    """"""
    Private helper to create and cache a Tokenizer instance based on canonical arguments.
    This function contains the expensive operations (get_encoding, Tokenizer instantiation).
    """"""
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)
    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )


def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]
) -> Tokenizer:
    """"""
    Gets a tokenizer, normalizing arguments to maximize cache hits.

    This public-facing function handles argument normalization before calling the
    cached helper. This ensures that different calls that should produce the
    same tokenizer (e.g., non-multilingual with different languages) result in a cache hit.
    """"""
    if not multilingual:
        # For non-multilingual tokenizers, language and task are not applicable.
        # This ensures all non-multilingual calls with the same num_languages are cached together.
        return _get_tokenizer_cached(
            encoding_name=""gpt2"",
            num_languages=num_languages,
            language=None,
            task=None,
        )

    # The rest of the function handles the multilingual case.
    effective_language = language
    if effective_language is not None:
        effective_language = effective_language.lower()
        if effective_language not in LANGUAGES:
            if effective_language in TO_LANGUAGE_CODE:
                effective_language = TO_LANGUAGE_CODE[effective_language]
            else:
                raise ValueError(f""Unsupported language: {language}"")

    return _get_tokenizer_cached(
        encoding_name=""multilingual"",
        num_languages=num_languages,
        language=effective_language or ""en"",
        task=task or ""transcribe"",
    )",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model.to(device)
","import os
import io
from typing import Optional, Union

import torch

def load_model(
    name: str,
    device: Optional[Union[str, torch.device]] = None,
    download_root: str = None,
    in_memory: bool = False,
) -> ""Whisper"":
    """"""
    Load a Whisper ASR model

    Parameters
    ----------
    name : str
        one of the official model names listed by `whisper.available_models()`, or
        path to a model checkpoint containing the model dimensions and the model state_dict.
    device : Union[str, torch.device]
        the PyTorch device to put the model into
    download_root: str
        path to download the model files; by default, it uses ""~/.cache/whisper""
    in_memory: bool
        whether to preload the model weights into host memory

    Returns
    -------
    model : Whisper
        The Whisper ASR model instance
    """"""
    if device is None:
        device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    if download_root is None:
        default = os.path.join(os.path.expanduser(""~""), "".cache"")
        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")

    if name in _MODELS:
        checkpoint_file = _download(_MODELS[name], download_root, in_memory)
        alignment_heads = _ALIGNMENT_HEADS[name]
    elif os.path.isfile(name):
        checkpoint_file = open(name, ""rb"").read() if in_memory else name
        alignment_heads = None
    else:
        raise RuntimeError(
            f""Model {name} not found; available models = {available_models()}""
        )

    with (
        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")
    ) as fp:
        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}
        checkpoint = torch.load(fp, map_location=device, **kwargs)
    del checkpoint_file

    dims = ModelDimensions(**checkpoint[""dims""])
    model = Whisper(dims)

    # Move model to the target device before loading the state dictionary
    model.to(device)
    model.load_state_dict(checkpoint[""model_state_dict""])

    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)

    return model.to(device)",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def encode(self, text, **kwargs):

    return self.encoding.encode(text, **kwargs)
","def encode(self, text, **kwargs):
    return self.encoding.encode(text, **kwargs)",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def __init__(self, dims: ModelDimensions):

    super().__init__()

    self.dims = dims

    self.encoder = AudioEncoder(

        self.dims.n_mels,

        self.dims.n_audio_ctx,

        self.dims.n_audio_state,

        self.dims.n_audio_head,

        self.dims.n_audio_layer,

    )

    self.decoder = TextDecoder(

        self.dims.n_vocab,

        self.dims.n_text_ctx,

        self.dims.n_text_state,

        self.dims.n_text_head,

        self.dims.n_text_layer,

    )

    # use the last half among the decoder layers for time alignment by default;

    # to use a specific set of heads, see `set_alignment_heads()` below.

    all_heads = torch.zeros(

        self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool

    )

    all_heads[self.dims.n_text_layer // 2 :] = True

    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)
","def __init__(self, dims: ModelDimensions):
    super().__init__()

    self.dims = dims
    self.encoder = AudioEncoder(
        self.dims.n_mels,
        self.dims.n_audio_ctx,
        self.dims.n_audio_state,
        self.dims.n_audio_head,
        self.dims.n_audio_layer,
    )
    self.decoder = TextDecoder(
        self.dims.n_vocab,
        self.dims.n_text_ctx,
        self.dims.n_text_state,
        self.dims.n_text_head,
        self.dims.n_text_layer,
    )

    # Create a sparse tensor for alignment heads directly to save memory.
    # This tensor marks the heads in the latter half of the decoder layers as active.
    start_layer = self.dims.n_text_layer // 2
    layer_indices, head_indices = torch.meshgrid(
        torch.arange(start_layer, self.dims.n_text_layer),
        torch.arange(self.dims.n_text_head),
        indexing=""ij"",
    )

    # Define the indices and values for the sparse tensor
    indices = torch.stack([layer_indices.flatten(), head_indices.flatten()])
    values = torch.ones(indices.shape[1], dtype=torch.bool)

    # Construct the sparse tensor
    alignment_heads = torch.sparse_coo_tensor(
        indices,
        values,
        (self.dims.n_text_layer, self.dims.n_text_head),
    )

    self.register_buffer(""alignment_heads"", alignment_heads, persistent=False)",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of given width along the last dimension of x""""""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y
","def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of given width along the last dimension of x.

    Optimized to use native PyTorch operations for better performance,
    simplicity, and maintainability by removing the custom CUDA kernel.
    """"""
    # Ensure the dimension to be unfolded is contiguous.
    if not x.is_contiguous():
        x = x.contiguous()

    # Create a view of the tensor with sliding windows of size `filter_width`.
    # This operation is very efficient as it doesn't copy data.
    slices = x.unfold(-1, filter_width, 1)

    # Compute the median over the last dimension (the window itself).
    # torch.median is highly optimized for both CPU and GPU execution.
    median, _ = torch.median(slices, dim=-1)

    return median",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,1,12.9304
"@lru_cache(maxsize=None)

def get_encoding(name: str = ""gpt2"", num_languages: int = 99):

    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")

    ranks = {

        base64.b64decode(token): int(rank)

        for token, rank in (line.split() for line in open(vocab_path) if line)

    }

    n_vocab = len(ranks)

    special_tokens = {}



    specials = [

        ""<|endoftext|>"",

        ""<|startoftranscript|>"",

        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],

        ""<|translate|>"",

        ""<|transcribe|>"",

        ""<|startoflm|>"",

        ""<|startofprev|>"",

        ""<|nospeech|>"",

        ""<|notimestamps|>"",

        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],

    ]



    for token in specials:

        special_tokens[token] = n_vocab

        n_vocab += 1



    return tiktoken.Encoding(

        name=os.path.basename(vocab_path),

        explicit_n_vocab=n_vocab,

        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",

        mergeable_ranks=ranks,

        special_tokens=special_tokens,

    )
","import os
import base64
from functools import lru_cache
import tiktoken

LANGUAGES = {
    ""en"": ""english"", ""zh"": ""chinese"", ""de"": ""german"", ""es"": ""spanish"", ""ru"": ""russian"",
    ""ko"": ""korean"", ""fr"": ""french"", ""ja"": ""japanese"", ""pt"": ""portuguese"", ""tr"": ""turkish"",
    ""pl"": ""polish"", ""ca"": ""catalan"", ""nl"": ""dutch"", ""ar"": ""arabic"", ""sv"": ""swedish"",
    ""it"": ""italian"", ""id"": ""indonesian"", ""hi"": ""hindi"", ""fi"": ""finnish"", ""vi"": ""vietnamese"",
    ""he"": ""hebrew"", ""uk"": ""ukrainian"", ""el"": ""greek"", ""ms"": ""malay"", ""cs"": ""czech"",
    ""ro"": ""romanian"", ""da"": ""danish"", ""hu"": ""hungarian"", ""ta"": ""tamil"", ""no"": ""norwegian"",
    ""th"": ""thai"", ""ur"": ""urdu"", ""hr"": ""croatian"", ""bg"": ""bulgarian"", ""lt"": ""lithuanian"",
    ""la"": ""latin"", ""mi"": ""maori"", ""ml"": ""malayalam"", ""cy"": ""welsh"", ""sk"": ""slovak"",
    ""te"": ""telugu"", ""fa"": ""persian"", ""lv"": ""latvian"", ""bn"": ""bengali"", ""sr"": ""serbian"",
    ""az"": ""azerbaijani"", ""sl"": ""slovenian"", ""kn"": ""kannada"", ""et"": ""estonian"", ""mk"": ""macedonian"",
    ""br"": ""breton"", ""eu"": ""basque"", ""is"": ""icelandic"", ""hy"": ""armenian"", ""ne"": ""nepali"",
    ""mn"": ""mongolian"", ""bs"": ""bosnian"", ""kk"": ""kazakh"", ""sq"": ""albanian"", ""sw"": ""swahili"",
    ""gl"": ""galician"", ""mr"": ""marathi"", ""pa"": ""punjabi"", ""si"": ""sinhala"", ""km"": ""khmer"",
    ""sn"": ""shona"", ""yo"": ""yoruba"", ""so"": ""somali"", ""af"": ""afrikaans"", ""oc"": ""occitan"",
    ""ka"": ""georgian"", ""be"": ""belarusian"", ""tg"": ""tajik"", ""sd"": ""sindhi"", ""gu"": ""gujarati"",
    ""am"": ""amharic"", ""yi"": ""yiddish"", ""lo"": ""lao"", ""uz"": ""uzbek"", ""fo"": ""faroese"",
    ""ht"": ""haitian creole"", ""ps"": ""pashto"", ""tk"": ""turkmen"", ""nn"": ""nynorsk"", ""mt"": ""maltese"",
    ""sa"": ""sanskrit"", ""lb"": ""luxembourgish"", ""my"": ""myanmar"", ""bo"": ""tibetan"", ""tl"": ""tagalog"",
    ""mg"": ""malagasy"", ""as"": ""assamese"", ""tt"": ""tatar"", ""haw"": ""hawaiian"", ""ln"": ""lingala"",
    ""ha"": ""hausa"", ""ba"": ""bashkir"", ""jw"": ""javanese"", ""su"": ""sundanese"",
}

def _load_ranks_from_file(vocab_path):
    with open(vocab_path, ""r"", encoding=""utf-8"") as f:
        return {
            base64.b64decode(token): int(rank)
            for token, rank in (line.split() for line in f if line)
        }

_RANKS_CACHE = {}

def _get_ranks(name: str):
    if name not in _RANKS_CACHE:
        vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")
        _RANKS_CACHE[name] = _load_ranks_from_file(vocab_path)
    return _RANKS_CACHE[name]

_ALL_LANGUAGE_TOKENS = tuple(f""<|{lang}|>"" for lang in LANGUAGES.keys())
_TIMESTAMP_TOKENS = tuple(f""<|{i * 0.02:.2f}|>"" for i in range(1501))

@lru_cache(maxsize=None)
def get_encoding(name: str = ""gpt2"", num_languages: int = 99):
    ranks = _get_ranks(name)
    n_vocab = len(ranks)

    language_tokens = _ALL_LANGUAGE_TOKENS[:num_languages]
    all_specials = (
        (""<|endoftext|>"", ""<|startoftranscript|>"")
        + language_tokens
        + (
            ""<|translate|>"",
            ""<|transcribe|>"",
            ""<|startoflm|>"",
            ""<|startofprev|>"",
            ""<|nospeech|>"",
            ""<|notimestamps|>"",
        )
        + _TIMESTAMP_TOKENS
    )

    special_tokens = {token: n_vocab + i for i, token in enumerate(all_specials)}
    final_n_vocab = n_vocab + len(all_specials)

    return tiktoken.Encoding(
        name=name,
        explicit_n_vocab=final_n_vocab,
        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )
",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
","import os
import hashlib
import urllib.request
import warnings
from typing import Union
from tqdm import tqdm

def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    """"""Downloads a file from a URL, verifies its checksum, and returns its contents or path.""""""
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))
    
    # A consistent chunk size for streaming I/O
    chunk_size = 8192

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")

    # Check if file already exists and has the correct checksum
    if os.path.isfile(download_target):
        hasher = hashlib.sha256()
        with open(download_target, ""rb"") as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        
        if hasher.hexdigest() == expected_sha256:
            if in_memory:
                with open(download_target, ""rb"") as f:
                    return f.read()
            return download_target
        else:
            warnings.warn(
                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
            )

    # Download the file, streaming the content to disk and calculating the checksum simultaneously
    hasher = hashlib.sha256()
    with urllib.request.urlopen(url) as source:
        total_size = int(source.info().get(""Content-Length"", 0))
        with open(download_target, ""wb"") as output:
            with tqdm(
                total=total_size,
                ncols=80,
                unit=""iB"",
                unit_scale=True,
                unit_divisor=1024,
            ) as loop:
                while True:
                    buffer = source.read(chunk_size)
                    if not buffer:
                        break

                    output.write(buffer)
                    hasher.update(buffer)
                    loop.update(len(buffer))

    # Verify the checksum of the newly downloaded file
    if hasher.hexdigest() != expected_sha256:
        os.remove(download_target) # Clean up corrupted file
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    # Return the file content or path as requested
    if in_memory:
        with open(download_target, ""rb"") as f:
            return f.read()
            
    return download_target",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
","import os
import hashlib
import urllib.request
import warnings
from typing import Union
from tqdm import tqdm

def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    """"""Downloads a file from a URL, verifies its checksum, and returns its contents or path.""""""
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))
    
    # A consistent chunk size for streaming I/O
    chunk_size = 8192

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")

    # Check if file already exists and has the correct checksum
    if os.path.isfile(download_target):
        hasher = hashlib.sha256()
        with open(download_target, ""rb"") as f:
            while chunk := f.read(chunk_size):
                hasher.update(chunk)
        
        if hasher.hexdigest() == expected_sha256:
            if in_memory:
                with open(download_target, ""rb"") as f:
                    return f.read()
            return download_target
        else:
            warnings.warn(
                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
            )

    # Download the file, streaming the content to disk and calculating the checksum simultaneously
    hasher = hashlib.sha256()
    with urllib.request.urlopen(url) as source:
        total_size = int(source.info().get(""Content-Length"", 0))
        with open(download_target, ""wb"") as output:
            with tqdm(
                total=total_size,
                ncols=80,
                unit=""iB"",
                unit_scale=True,
                unit_divisor=1024,
            ) as loop:
                while True:
                    buffer = source.read(chunk_size)
                    if not buffer:
                        break

                    output.write(buffer)
                    hasher.update(buffer)
                    loop.update(len(buffer))

    # Verify the checksum of the newly downloaded file
    if hasher.hexdigest() != expected_sha256:
        os.remove(download_target) # Clean up corrupted file
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    # Return the file content or path as requested
    if in_memory:
        with open(download_target, ""rb"") as f:
            return f.read()
            
    return download_target",whisper,25,"Let's optimize the following code step by step:

Please follow these reasoning steps:
1. First, analyze the current code to identify performance bottlenecks
2. Consider different optimization strategies (algorithmic, data structure, loop optimization, etc.)
3. Evaluate the trade-offs of each approach
4. Select the best optimization strategy
5. Implement the optimized version

Think through each step, then provide only the final optimized code.",COT,0,12.9304
"def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)

        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]



    if ndim <= 2:

        result = result[0, 0]



    return result
","def median_filter(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""
    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), ""`filter_width` should be an odd number""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    if (ndim := x.ndim) <= 2:
        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D
        x = x[None, None, :]

    x = F.pad(x, (pad_width, pad_width, 0, 0), mode=""reflect"")

    result = None
    if x.is_cuda:
        try:
            from .triton_ops import median_filter_cuda

            result = median_filter_cuda(x, filter_width)
        except (RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""
                ""falling back to a slower median kernel implementation...""
            )

    if result is None:
        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)
        result = x.unfold(-1, filter_width, 1).sort()[0][..., pad_width]

    if ndim <= 2:
        result = result[0, 0]

    return result",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel



    M, N = x.shape

    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""



    x_skew = (

        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)

    )

    x_skew = x_skew.T.contiguous()

    cost = torch.ones(N + M + 2, M + 2) * np.inf

    cost[0, 0] = 0

    cost = cost.to(x.device)

    trace = torch.zeros_like(cost, dtype=torch.int32)



    dtw_kernel[(1,)](

        cost,

        trace,

        x_skew,

        x_skew.stride(0),

        cost.stride(0),

        trace.stride(0),

        N,

        M,

        BLOCK_SIZE=BLOCK_SIZE,

    )



    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[

        :, : N + 1

    ]

    return backtrace(trace.cpu().numpy())
","def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""

    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
    )
    x_skew = x_skew.T.contiguous()

    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), device=x.device, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[
        :, : N + 1
    ]
    return backtrace(trace.cpu().numpy())",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"@lru_cache(maxsize=None)

def get_tokenizer(

    multilingual: bool,

    *,

    num_languages: int = 99,

    language: Optional[str] = None,

    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]

) -> Tokenizer:

    if language is not None:

        language = language.lower()

        if language not in LANGUAGES:

            if language in TO_LANGUAGE_CODE:

                language = TO_LANGUAGE_CODE[language]

            else:

                raise ValueError(f""Unsupported language: {language}"")



    if multilingual:

        encoding_name = ""multilingual""

        language = language or ""en""

        task = task or ""transcribe""

    else:

        encoding_name = ""gpt2""

        language = None

        task = None



    encoding = get_encoding(name=encoding_name, num_languages=num_languages)



    return Tokenizer(

        encoding=encoding, num_languages=num_languages, language=language, task=task

    )
","@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]
) -> Tokenizer:
    if language is not None:
        lang_lower = language.lower()
        if lang_lower in LANGUAGES:
            language = lang_lower
        else:
            language = TO_LANGUAGE_CODE.get(lang_lower)
            if language is None:
                # Use the lowercased input in the error message, consistent with original
                raise ValueError(f""Unsupported language: {lang_lower}"")

    if multilingual:
        encoding_name = ""multilingual""
        language = language or ""en""
        task = task or ""transcribe""
    else:
        encoding_name = ""gpt2""
        language = None
        task = None

    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model.to(device)
","def load_model(
    name: str,
    device: Optional[Union[str, torch.device]] = None,
    download_root: str = None,
    in_memory: bool = False,
) -> Whisper:
    """"""
    Load a Whisper ASR model

    Parameters
    ----------
    name : str
        one of the official model names listed by `whisper.available_models()`, or
        path to a model checkpoint containing the model dimensions and the model state_dict.
    device : Union[str, torch.device]
        the PyTorch device to put the model into
    download_root: str
        path to download the model files; by default, it uses ""~/.cache/whisper""
    in_memory: bool
        whether to preload the model weights into host memory

    Returns
    -------
    model : Whisper
        The Whisper ASR model instance
    """"""
    if device is None:
        device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    if download_root is None:
        default = os.path.join(os.path.expanduser(""~""), "".cache"")
        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")

    if name in _MODELS:
        checkpoint_file = _download(_MODELS[name], download_root, in_memory)
        alignment_heads = _ALIGNMENT_HEADS[name]
    elif os.path.isfile(name):
        if in_memory:
            with open(name, ""rb"") as f:
                checkpoint_file = f.read()
        else:
            checkpoint_file = name
        alignment_heads = None
    else:
        raise RuntimeError(
            f""Model {name} not found; available models = {available_models()}""
        )

    with (
        io.BytesIO(checkpoint_file) if isinstance(checkpoint_file, bytes) else open(checkpoint_file, ""rb"")
    ) as fp:
        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}
        # Load checkpoint to CPU memory first to avoid unnecessary GPU <-> CPU transfers.
        checkpoint = torch.load(fp, map_location=""cpu"", **kwargs)
    del checkpoint_file

    dims = ModelDimensions(**checkpoint[""dims""])
    model = Whisper(dims)
    model.load_state_dict(checkpoint[""model_state_dict""])

    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)

    # Move the fully constructed model to the target device.
    return model.to(device)",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def encode(self, text, **kwargs):

    return self.encoding.encode(text, **kwargs)
","def encode(self, text, **kwargs):
    return self.encoding.encode(text, **kwargs)",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def __init__(self, dims: ModelDimensions):

    super().__init__()

    self.dims = dims

    self.encoder = AudioEncoder(

        self.dims.n_mels,

        self.dims.n_audio_ctx,

        self.dims.n_audio_state,

        self.dims.n_audio_head,

        self.dims.n_audio_layer,

    )

    self.decoder = TextDecoder(

        self.dims.n_vocab,

        self.dims.n_text_ctx,

        self.dims.n_text_state,

        self.dims.n_text_head,

        self.dims.n_text_layer,

    )

    # use the last half among the decoder layers for time alignment by default;

    # to use a specific set of heads, see `set_alignment_heads()` below.

    all_heads = torch.zeros(

        self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool

    )

    all_heads[self.dims.n_text_layer // 2 :] = True

    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)
","def __init__(self, dims: ModelDimensions):
    super().__init__()
    self.dims = dims
    self.encoder = AudioEncoder(
        self.dims.n_mels,
        self.dims.n_audio_ctx,
        self.dims.n_audio_state,
        self.dims.n_audio_head,
        self.dims.n_audio_layer,
    )
    self.decoder = TextDecoder(
        self.dims.n_vocab,
        self.dims.n_text_ctx,
        self.dims.n_text_state,
        self.dims.n_text_head,
        self.dims.n_text_layer,
    )

    # use the last half among the decoder layers for time alignment by default;
    # to use a specific set of heads, see `set_alignment_heads()` below.
    n_layers = self.dims.n_text_layer
    n_heads = self.dims.n_text_head
    start_layer = n_layers // 2

    # Directly create the sparse indices and values to avoid allocating
    # a dense intermediate tensor, which is more memory-efficient.
    layer_indices, head_indices = torch.meshgrid(
        torch.arange(start_layer, n_layers),
        torch.arange(n_heads),
        indexing=""ij"",
    )
    indices = torch.stack([layer_indices.flatten(), head_indices.flatten()])
    values = torch.ones(indices.shape[1], dtype=torch.bool)

    alignment_heads = torch.sparse_coo_tensor(
        indices, values, size=(n_layers, n_heads)
    )
    self.register_buffer(""alignment_heads"", alignment_heads, persistent=False)",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of given width along the last dimension of x""""""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y
","def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of given width along the last dimension of x""""""
    import math

    x = x.contiguous()

    # Directly calculate the output shape without creating an intermediate unfolded tensor.
    # This avoids the overhead of the `unfold` operation and is more memory efficient.
    output_last_dim = max(0, x.shape[-1] - filter_width + 1)
    output_shape = x.shape[:-1] + (output_last_dim,)
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device)

    if y.numel() == 0:
        return y

    # Calculate grid size using math.prod, which is more direct than using the shape
    # of a temporary tensor and avoids a dependency on numpy.
    grid = math.prod(x.shape[:-1])

    kernel = median_kernel(filter_width)

    # The block size calculation and kernel launch signature are preserved as they are
    # tightly coupled with the specific CUDA kernel implementation.
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,1,13.636599999999998
"@lru_cache(maxsize=None)

def get_encoding(name: str = ""gpt2"", num_languages: int = 99):

    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")

    ranks = {

        base64.b64decode(token): int(rank)

        for token, rank in (line.split() for line in open(vocab_path) if line)

    }

    n_vocab = len(ranks)

    special_tokens = {}



    specials = [

        ""<|endoftext|>"",

        ""<|startoftranscript|>"",

        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],

        ""<|translate|>"",

        ""<|transcribe|>"",

        ""<|startoflm|>"",

        ""<|startofprev|>"",

        ""<|nospeech|>"",

        ""<|notimestamps|>"",

        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],

    ]



    for token in specials:

        special_tokens[token] = n_vocab

        n_vocab += 1



    return tiktoken.Encoding(

        name=os.path.basename(vocab_path),

        explicit_n_vocab=n_vocab,

        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",

        mergeable_ranks=ranks,

        special_tokens=special_tokens,

    )
","@lru_cache(maxsize=None)
def get_encoding(name: str = ""gpt2"", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")
    with open(vocab_path, ""r"", encoding=""utf-8"") as f:
        ranks = {
            base64.b64decode(token): int(rank)
            for token, rank in (line.split() for line in f if line)
        }
    n_vocab = len(ranks)
    specials = [
        ""<|endoftext|>"",
        ""<|startoftranscript|>"",
        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],
        ""<|translate|>"",
        ""<|transcribe|>"",
        ""<|startoflm|>"",
        ""<|startofprev|>"",
        ""<|nospeech|>"",
        ""<|notimestamps|>"",
        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],
    ]
    special_tokens = {token: n_vocab + i for i, token in enumerate(specials)}
    explicit_n_vocab = n_vocab + len(specials)
    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=explicit_n_vocab,
        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,1,13.636599999999998
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
","def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")

    if os.path.isfile(download_target):
        hasher = hashlib.sha256()
        with open(download_target, ""rb"") as f:
            while chunk := f.read(8192):
                hasher.update(chunk)
        
        if hasher.hexdigest() == expected_sha256:
            if in_memory:
                with open(download_target, ""rb"") as f:
                    return f.read()
            return download_target
        else:
            warnings.warn(
                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
            )

    hasher = hashlib.sha256()
    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:
        content_length = source.info().get(""Content-Length"")
        total = int(content_length) if content_length is not None else None

        with tqdm(
            total=total,
            ncols=80,
            unit=""iB"",
            unit_scale=True,
            unit_divisor=1024,
        ) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                hasher.update(buffer)
                loop.update(len(buffer))

    if hasher.hexdigest() != expected_sha256:
        os.remove(download_target)
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""
        )

    if in_memory:
        with open(download_target, ""rb"") as f:
            return f.read()
            
    return download_target",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
"def load_audio(file: str, sr: int = SAMPLE_RATE):

    """"""

    Open an audio file and read as mono waveform, resampling as necessary



    Parameters

    ----------

    file: str

        The audio file to open



    sr: int

        The sample rate to resample the audio if necessary



    Returns

    -------

    A NumPy array containing the audio waveform, in float32 dtype.

    """"""



    # This launches a subprocess to decode audio while down-mixing

    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.

    # fmt: off

    cmd = [

        ""ffmpeg"",

        ""-nostdin"",

        ""-threads"", ""0"",

        ""-i"", file,

        ""-f"", ""s16le"",

        ""-ac"", ""1"",

        ""-acodec"", ""pcm_s16le"",

        ""-ar"", str(sr),

        ""-""

    ]

    # fmt: on

    try:

        out = run(cmd, capture_output=True, check=True).stdout

    except CalledProcessError as e:

        raise RuntimeError(f""Failed to load audio: {e.stderr.decode()}"") from e



    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0
","def load_audio(file: str, sr: int = SAMPLE_RATE):
    """"""
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """"""
    try:
        # Use PyAV to decode audio in-process, avoiding the overhead of
        # launching a subprocess for ffmpeg.
        with av.open(file, ""r"") as container:
            # Select the first audio stream
            stream = container.streams.get(audio=0)[0]

            # Set up the resampler to convert the audio to the target format
            resampler = av.AudioResampler(
                format=""s16"",    # 16-bit signed integer PCM
                layout=""mono"",   # Single channel
                rate=sr,         # Target sample rate
            )

            # Create a buffer to accumulate the resampled audio data efficiently
            buffer = io.BytesIO()

            # Decode and resample all frames
            for frame in container.decode(stream):
                for resampled_frame in resampler.resample(frame):
                    buffer.write(resampled_frame.planes[0].to_bytes())

            # Convert the accumulated bytes to a NumPy array
            raw_audio = buffer.getvalue()
            audio_np = np.frombuffer(raw_audio, dtype=np.int16)

            # Normalize to float32 in the range [-1.0, 1.0)
            return audio_np.astype(np.float32) / 32768.0

    except (av.error.EOFError, StopIteration):
        # Handle empty or corrupted files that result in no audio frames
        return np.empty(0, dtype=np.float32)
    except Exception as e:
        raise RuntimeError(f""Failed to load audio: {e}"") from e",whisper,25,"You are an expert in code optimization. Please optimize the provided code for Optimize the specific code object provided. Return ONLY the optimized version of that object, preserving its exact signature and interface. Do not recreate parent classes or surrounding code.. Consider the project context, task context, and adapt your optimization approach accordingly.

## Project Context
Project Name: whisper
Project Description: Robust Speech Recognition via Large-Scale Weak Supervision
Primary Languages: Python

## Task Context
- Description: Synthesize a single, best-runtime optimized version of the given object, preserving its signature.
- Considerations: Algorithmic complexity and big O notation; data structures and their efficiency; loop optimizations and redundant iterations; memory access patterns and cache utilization; I/O operations and system calls; parallel processing and multi-threading; redundant computations

## Target LLM Context
- Target Model: gemini-v25-pro
- Considerations: Apply complex reasoning to verify assumptions about performance metrics and project goals, think step by step to analyze bottlenecks evaluate trade offs and select the best strategy, provide only the final optimized code after internal reasoning",BASE,0,13.636599999999998
