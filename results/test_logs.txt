Reinitializing whisper at correct path...
Running py-spy profiler...
GENERATED META PROMPT: 
Optimize the given Python code object from the "whisper" project for improved runtime performance, ensuring that the exact signature and interface are preserved. When optimizing, consider algorithmic complexity, efficient use of data structures, loop and redundant iteration minimization, memory access and cache patterns, I/O operations, and potential parallelization. Document the reasoning behind each optimization choice, balancing micro-level enhancements with overall structural integrity while maintaining code readability and maintainability suitable for the capabilities of claude-sonnet-4. Return only the optimized version of the object.
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 128
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"
    
    # Optimization 1: Use torch.full instead of torch.ones + multiplication
    # Reasoning: torch.full directly creates tensor with specified value, avoiding extra multiplication
    
    # Optimization 2: Pre-allocate tensors on target device to avoid device transfers
    # Reasoning: Creates tensors directly on GPU, eliminating CPU->GPU transfer overhead
    device = x.device
    
    # Optimization 3: Calculate dimensions once and reuse
    # Reasoning: Avoid redundant arithmetic operations in tensor operations
    N_plus_M = N + M
    N_plus_M_plus_2 = N_plus_M + 2
    M_plus_2 = M + 2
    M_plus_1 = M + 1
    N_plus_1 = N + 1
    
    # Optimization 4: Streamlined skewing operation with minimal memory allocations
    # Reasoning: Reduces intermediate tensor creation and memory fragmentation
    # Original approach created multiple intermediate tensors; this minimizes allocations
    x_skew = torch.full((M, N_plus_M), float('inf'), device=device, dtype=x.dtype)
    x_skew[:, :N] = x
    x_skew = x_skew.T.contiguous()
    
    # Optimization 5: Direct device allocation for cost and trace matrices
    # Reasoning: Eliminates CPU tensor creation and subsequent device transfer
    cost = torch.full((N_plus_M_plus_2, M_plus_2), float('inf'), device=device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros((N_plus_M_plus_2, M_plus_2), device=device, dtype=torch.int32)
    
    # Optimization 6: Pre-compute strides to avoid repeated stride calculations
    # Reasoning: Stride computation can be expensive; caching improves performance
    x_skew_stride_0 = x_skew.stride(0)
    cost_stride_0 = cost.stride(0)
    trace_stride_0 = trace.stride(0)
    
    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew_stride_0,
        cost_stride_0,
        trace_stride_0,
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    
    # Optimization 7: Efficient trace extraction with minimal intermediate operations
    # Reasoning: Reduces memory allocations and improves cache locality
    # Direct slicing approach minimizes temporary tensor creation
    trace_transposed = trace.T
    
    # Optimization 8: Calculate slice bounds once to avoid redundant computation
    # Reasoning: Eliminates repeated arithmetic in slice operations
    trace_flat_size = M_plus_1 * (M + N + 3)
    trace_reshaped_cols = M + N + 3
    
    trace_result = (trace_transposed
                   .flatten()[:trace_flat_size]
                   .reshape(M_plus_1, trace_reshaped_cols)[:, :N_plus_1])
    
    # Optimization 9: Single CPU transfer at the end
    # Reasoning: Minimizes expensive GPU->CPU transfers by doing it only once
    return backtrace(trace_result.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,27) (2,23)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,42) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1579) (2,1535)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,394) (2,300)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.63s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1450 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\tokenizer.py, startline : 329
@lru_cache(maxsize=None)
def get_encoding(name: str = "gpt2", num_languages: int = 99):
    # Optimization 1: Use pathlib for more efficient path operations
    vocab_path = Path(__file__).parent / "assets" / f"{name}.tiktoken"
    
    # Optimization 2: Use context manager and optimize file reading
    # Read entire file at once and split, avoiding line-by-line iteration
    with open(vocab_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    # Optimization 3: Pre-split content and filter empty lines in single pass
    lines = [line for line in content.split('\n') if line]
    
    # Optimization 4: Use dict comprehension with optimized parsing
    # Avoid repeated calls to split() by using maxsplit=1
    ranks = {
        base64.b64decode(token): int(rank)
        for line in lines
        for token, rank in [line.split(None, 1)]
    }
    
    n_vocab = len(ranks)
    
    # Optimization 5: Pre-calculate language keys slice to avoid repeated lookups
    lang_keys = list(LANGUAGES.keys())[:num_languages]
    
    # Optimization 6: Pre-calculate timestamp tokens using list comprehension
    # and format optimization (avoid repeated format calls)
    timestamp_tokens = [f"<|{i * 0.02:.2f}|>" for i in range(1501)]
    
    # Optimization 7: Build specials list more efficiently
    # Separate static tokens from dynamic ones to minimize list operations
    static_specials = [
        "<|endoftext|>",
        "<|startoftranscript|>",
        "<|translate|>", 
        "<|transcribe|>",
        "<|startoflm|>",
        "<|startofprev|>",
        "<|nospeech|>",
        "<|notimestamps|>",
    ]
    
    # Optimization 8: Use generator expressions to minimize intermediate lists
    lang_specials = (f"<|{lang}|>" for lang in lang_keys)
    
    # Optimization 9: Build special_tokens dict in single pass using enumerate
    # This eliminates the need for manual counter increment
    all_specials = static_specials + list(lang_specials) + timestamp_tokens
    special_tokens = {token: n_vocab + idx for idx, token in enumerate(all_specials)}
    
    # Update n_vocab count
    n_vocab += len(all_specials)
    
    # Optimization 10: Cache basename result to avoid repeated path operations
    encoding_name = vocab_path.name
    
    return tiktoken.Encoding(
        name=encoding_name,
        explicit_n_vocab=n_vocab,
        pat_str=r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ................                   [ 80%]
projects\whisper\tests\test_tokenizer.py FFFF                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
____________________________ test_tokenizer[True] _____________________________
projects\whisper\tests\test_tokenizer.py:8: in test_tokenizer
    tokenizer = get_tokenizer(multilingual=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:407: in get_tokenizer
    encoding = get_encoding(name="gpt2", num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:333: in get_encoding
    vocab_path = Path(__file__).parent / "assets" / f"{name}.tiktoken"
                 ^^^^
E   NameError: name 'Path' is not defined
____________________________ test_tokenizer[False] ____________________________
projects\whisper\tests\test_tokenizer.py:8: in test_tokenizer
    tokenizer = get_tokenizer(multilingual=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:407: in get_tokenizer
    encoding = get_encoding(name="gpt2", num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:333: in get_encoding
    vocab_path = Path(__file__).parent / "assets" / f"{name}.tiktoken"
                 ^^^^
E   NameError: name 'Path' is not defined
_________________________ test_multilingual_tokenizer _________________________
projects\whisper\tests\test_tokenizer.py:15: in test_multilingual_tokenizer
    gpt2_tokenizer = get_tokenizer(multilingual=False)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:407: in get_tokenizer
    encoding = get_encoding(name="gpt2", num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:333: in get_encoding
    vocab_path = Path(__file__).parent / "assets" / f"{name}.tiktoken"
                 ^^^^
E   NameError: name 'Path' is not defined
____________________________ test_split_on_unicode ____________________________
projects\whisper\tests\test_tokenizer.py:28: in test_split_on_unicode
    multilingual_tokenizer = get_tokenizer(multilingual=True)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:435: in get_tokenizer
    encoding = get_encoding(name="multilingual", num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:333: in get_encoding
    vocab_path = Path(__file__).parent / "assets" / f"{name}.tiktoken"
                 ^^^^
E   NameError: name 'Path' is not defined
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_tokenizer.py::test_tokenizer[True] - NameE...
FAILED projects\whisper\tests\test_tokenizer.py::test_tokenizer[False] - Name...
FAILED projects\whisper\tests\test_tokenizer.py::test_multilingual_tokenizer
FAILED projects\whisper\tests\test_tokenizer.py::test_split_on_unicode - Name...
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.26s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile8.speedscope'. Samples: 1485 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 13.743
Running py-spy profiler...
Benchmark 2 complete with duration 13.489
Running py-spy profiler...
Benchmark 3 complete with duration 13.395
Running py-spy profiler...
Benchmark 4 complete with duration 13.593
Running py-spy profiler...
Benchmark 5 complete with duration 13.251
Running py-spy profiler...
Benchmark 6 complete with duration 13.641
Running py-spy profiler...
Benchmark 7 complete with duration 13.633
Running py-spy profiler...
Benchmark 8 complete with duration 13.384
Running py-spy profiler...
Benchmark 9 complete with duration 13.722
Running py-spy profiler...
Benchmark 10 complete with duration 13.428

Done with MP prompting - moving to next prompt type for project whisper with optimizer 40

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = slices.numel() // filter_width
    
    kernel = median_kernel(filter_width)
    y = torch.empty(slices.shape[:-1], dtype=x.dtype, device=x.device)
    
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                 ^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:153: in load_model
    checkpoint = torch.load(fp, map_location=device, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 12.30s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1346 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 14.003
Running py-spy profiler...
Benchmark 2 complete with duration 14.02
Running py-spy profiler...
Benchmark 3 complete with duration 13.955
Running py-spy profiler...
Benchmark 4 complete with duration 14.101
Running py-spy profiler...
Benchmark 5 complete with duration 14.283
Running py-spy profiler...
Benchmark 6 complete with duration 13.951
Running py-spy profiler...
Benchmark 7 complete with duration 14.107
Running py-spy profiler...
Benchmark 8 complete with duration 13.97
Running py-spy profiler...
Benchmark 9 complete with duration 14.142
Running py-spy profiler...
Benchmark 10 complete with duration 14.023

Done with FS prompting - moving to next prompt type for project whisper with optimizer 40

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-compute batch dimensions to avoid repeated calculations
    batch_shape = x.shape[:-1]
    seq_len = x.shape[-1]
    output_len = seq_len - filter_width + 1
    
    # Avoid unfold operation which creates memory copies
    # Instead, pass original tensor and let kernel handle indexing
    y = torch.empty(batch_shape + (output_len,), dtype=x.dtype, device=x.device)
    
    # Use power-of-2 block size for better GPU memory coalescing
    # Clamp between reasonable bounds for different GPU architectures
    BLOCK_SIZE = min(1024, max(32, 1 << (output_len - 1).bit_length()))
    
    # Calculate grid size based on total elements, not intermediate tensor shape
    total_elements = y.numel()
    grid_size = (total_elements + BLOCK_SIZE - 1) // BLOCK_SIZE
    
    # Get kernel once and reuse
    kernel = median_kernel(filter_width)
    
    # Launch kernel with optimized parameters
    kernel[(grid_size,)](
        y, 
        x, 
        filter_width,
        seq_len,
        total_elements,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.15s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1339 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-validate inputs
    if filter_width <= 0:
        raise ValueError("filter_width must be positive")
    if x.size(-1) < filter_width:
        raise ValueError("Input tensor's last dimension must be >= filter_width")
    
    # Use views instead of unfold for better memory efficiency
    input_size = x.size(-1)
    output_size = input_size - filter_width + 1
    
    # Create output tensor with correct shape
    output_shape = list(x.shape)
    output_shape[-1] = output_size
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device)
    
    # Calculate grid size more efficiently
    grid_size = y.numel() // output_size
    
    # Optimize block size calculation
    stride = x.stride(-2) if x.dim() >= 2 else 1
    BLOCK_SIZE = min(1024, max(32, 1 << (stride - 1).bit_length())) if stride > 0 else 256
    
    # Get or create kernel (assuming caching mechanism exists)
    kernel = median_kernel(filter_width)
    
    # Launch kernel with optimized parameters
    kernel[(grid_size,)](
        y, 
        x, 
        x.stride(-2) if x.dim() >= 2 else 1, 
        y.stride(-2) if y.dim() >= 2 else 1, 
        input_size,
        output_size,
        filter_width,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:135: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:135: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:135: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:135: in median_filter_cuda
    kernel[(grid_size,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 11.98s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1284 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-validate inputs to avoid unnecessary computation
    if filter_width <= 0:
        raise ValueError("filter_width must be positive")
    if filter_width == 1:
        return x.clone()
    if x.size(-1) < filter_width:
        raise ValueError("Input tensor last dimension smaller than filter width")
    
    # Use more efficient view operations and minimize memory allocations
    input_shape = x.shape
    last_dim_size = input_shape[-1]
    output_size = last_dim_size - filter_width + 1
    
    # Reshape for more efficient memory access patterns
    x_reshaped = x.contiguous().view(-1, last_dim_size)
    batch_size = x_reshaped.size(0)
    
    # Pre-allocate output with correct shape
    y = torch.empty(batch_size, output_size, dtype=x.dtype, device=x.device)
    
    # Optimize grid calculation and block size selection
    total_elements = batch_size * output_size
    
    # Use cached kernel to avoid recompilation overhead
    kernel = get_cached_median_kernel(filter_width)
    
    # Calculate optimal block size based on device properties and data size
    device_props = torch.cuda.get_device_properties(x.device)
    max_threads_per_block = device_props.max_threads_per_block
    
    # Choose block size as power of 2 for better memory coalescing
    optimal_block_size = min(
        max_threads_per_block,
        1 << (total_elements.bit_length() - 1) if total_elements > 0 else 256
    )
    BLOCK_SIZE = max(32, optimal_block_size)  # Ensure minimum warp size
    
    # Calculate grid size more efficiently
    grid_size = (total_elements + BLOCK_SIZE - 1) // BLOCK_SIZE
    
    # Launch kernel with optimized parameters
    kernel[grid_size, BLOCK_SIZE](
        y, x_reshaped, 
        last_dim_size, output_size, 
        filter_width, batch_size
    )
    
    # Reshape output back to original dimensions (except last dim)
    return y.view(*input_shape[:-1], output_size)

# Add kernel caching to avoid recompilation
_kernel_cache = {}

def get_cached_median_kernel(filter_width: int):
    """Get cached median kernel or create new one if not exists"""
    if filter_width not in _kernel_cache:
        _kernel_cache[filter_width] = median_kernel(filter_width)
    return _kernel_cache[filter_width]
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:137: in median_filter_cuda
    max_threads_per_block = device_props.max_threads_per_block
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'max_threads_per_block'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:137: in median_filter_cuda
    max_threads_per_block = device_props.max_threads_per_block
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'max_threads_per_block'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:137: in median_filter_cuda
    max_threads_per_block = device_props.max_threads_per_block
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'max_threads_per_block'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:137: in median_filter_cuda
    max_threads_per_block = device_props.max_threads_per_block
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'max_threads_per_block'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 11.98s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1231 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-allocate output tensor with correct shape
    output_shape = list(x.shape)
    output_shape[-1] = output_shape[-1] - filter_width + 1
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device)
    
    # Calculate grid size more efficiently
    grid = y.numel() // y.shape[-1]
    
    # Get or cache kernel
    kernel = median_kernel(filter_width)
    
    # Optimize block size calculation
    stride = x.stride(-2) if x.dim() > 1 else 1
    BLOCK_SIZE = min(1024, max(32, 1 << (stride - 1).bit_length()))
    
    # Launch kernel with flattened input handling
    kernel[(grid,)](
        y, 
        x, 
        x.stride(-2) if x.dim() > 1 else x.stride(-1), 
        y.stride(-2) if y.dim() > 1 else y.stride(-1), 
        filter_width,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:125: in median_filter_cuda
    kernel[(grid,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:125: in median_filter_cuda
    kernel[(grid,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:125: in median_filter_cuda
    kernel[(grid,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:44: in median_filter
    result = median_filter_cuda(x_padded, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:125: in median_filter_cuda
    kernel[(grid,)](
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() got multiple values for argument 'BLOCK_SIZE'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.10s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1331 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-validate inputs
    if filter_width <= 0 or filter_width > x.size(-1):
        raise ValueError(f"Invalid filter_width: {filter_width}")
    
    # Early return for trivial cases
    if filter_width == 1:
        return x
    
    # Use view instead of unfold when possible for better memory efficiency
    if x.is_contiguous():
        slices = x.unfold(-1, filter_width, 1)
    else:
        x = x.contiguous()
        slices = x.unfold(-1, filter_width, 1)
    
    # Calculate grid size more efficiently
    grid = slices.numel() // filter_width
    
    # Cache kernel creation
    if not hasattr(median_filter_cuda, '_kernel_cache'):
        median_filter_cuda._kernel_cache = {}
    
    if filter_width not in median_filter_cuda._kernel_cache:
        median_filter_cuda._kernel_cache[filter_width] = median_kernel(filter_width)
    
    kernel = median_filter_cuda._kernel_cache[filter_width]
    
    # Pre-allocate output tensor with optimal memory layout
    output_shape = slices.shape[:-1]
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device, 
                    memory_format=torch.contiguous_format)
    
    # Optimize block size calculation using bit operations
    stride = y.stride(-2) if y.dim() > 1 else 1
    BLOCK_SIZE = 1 << max(0, (stride - 1).bit_length())
    BLOCK_SIZE = min(BLOCK_SIZE, 1024)  # Cap at reasonable maximum
    
    # Launch kernel with optimized parameters
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                 ^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:168: in load_model
    checkpoint = torch.load(fp, map_location=device, **load_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 11.48s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1248 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Pre-validate inputs to avoid redundant operations
    if filter_width <= 0:
        raise ValueError("filter_width must be positive")
    if filter_width == 1:
        return x
    
    # Ensure tensor is on CUDA device and contiguous
    if not x.is_cuda:
        x = x.cuda()
    
    x_contiguous = x.contiguous()
    slices = x_contiguous.unfold(-1, filter_width, 1)
    
    # More efficient grid calculation using tensor operations
    batch_size = slices.numel() // slices.size(-1)
    
    # Pre-allocate output tensor with optimal memory layout
    y = torch.empty(slices.shape[:-1], dtype=x.dtype, device=x.device, 
                   memory_format=torch.contiguous_format)
    
    # Cache kernel to avoid recompilation
    if not hasattr(median_filter_cuda, '_kernel_cache'):
        median_filter_cuda._kernel_cache = {}
    
    cache_key = (filter_width, x.dtype)
    if cache_key not in median_filter_cuda._kernel_cache:
        median_filter_cuda._kernel_cache[cache_key] = median_kernel(filter_width)
    
    kernel = median_filter_cuda._kernel_cache[cache_key]
    
    # Optimize block size based on tensor characteristics and hardware
    optimal_block_size = min(1024, max(32, 
                            1 << (batch_size.bit_length() - 1)))
    
    # Launch kernel with optimized configuration
    kernel[(batch_size,)](y, x_contiguous, x_contiguous.stride(-2), 
                         y.stride(-2), BLOCK_SIZE=optimal_block_size)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                 ^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:168: in load_model
    checkpoint = torch.load(fp, map_location=device, **load_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 12.22s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1310 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

6 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Input validation
    if filter_width <= 0 or filter_width > x.size(-1):
        raise ValueError("Invalid filter width")
    
    # Early return for trivial cases
    if filter_width == 1:
        return x.clone()
    
    # Pre-allocate output tensor with exact size
    output_size = x.size(-1) - filter_width + 1
    y_shape = x.shape[:-1] + (output_size,)
    y = torch.empty(y_shape, dtype=x.dtype, device=x.device)
    
    # Only create unfold if we need it
    slices = x.contiguous().unfold(-1, filter_width, 1)
    
    # Optimize grid calculation - avoid np.prod overhead
    grid = 1
    for dim in slices.shape[:-2]:
        grid *= dim
    
    # Get kernel once
    kernel = median_kernel(filter_width)
    
    # Optimize block size calculation with bit manipulation
    stride = y.stride(-2) if y.dim() > 1 else 1
    BLOCK_SIZE = 1 << max(0, (stride - 1).bit_length())
    
    # Single kernel launch
    kernel[(grid,)](
        y, 
        slices, 
        slices.stride(-2), 
        y.stride(-2), 
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000022AC3186F30>(tensor([[[-1.3967, -0.5358, -0.6127,  ...,  0.2676,  0.3716,  0.3716],\n         [-0.0951, -0.0951,  0.0199,  ..., -1.9...23, -1.2273,  ..., -0.9124, -0.9124, -1.2469],\n         [ 0.9062,  0.3537,  0.9062,  ..., -1.0017, -0.2295, -1.9487]]]), tensor([[[-1.3967, -0.5358, -0.6127,  ...,  0.2676,  0.3716,  0.3716],\n         [-0.5358, -0.6127, -0.6127,  ...,  0.3...43,  0.0099,  ...,  0.2478,  0.2478,  0.2972],\n         [-0.0043,  0.0099, -0.0535,  ...,  0.2478,  0.2972,  0.2972]]]))
E    +    where <function allclose at 0x0000022AC3186F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000022AC3186F30>(tensor([[[[ 2.0326e-01,  2.0326e-01,  2.0326e-01,  ...,  2.4347e-01,\n           -3.6045e-01, -6.5978e-01],\n          [...048e+00],\n          [ 4.0631e-01,  4.0631e-01,  4.0631e-01,  ..., -9.7981e-01,\n           -1.0776e+00, -5.6604e-01]]]]), tensor([[[[ 2.0326e-01,  2.0326e-01,  2.0326e-01,  ...,  2.4347e-01,\n           -3.6045e-01, -6.5978e-01],\n          [...887e-01],\n          [ 3.5140e-01,  3.5140e-01, -9.4030e-01,  ...,  1.5610e+00,\n            2.9887e-01,  1.3839e-01]]]]))
E    +    where <function allclose at 0x0000022AC3186F30> = np.allclose
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 13.44s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1340 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

7 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 12.871
Running py-spy profiler...
Benchmark 2 complete with duration 12.844
Running py-spy profiler...
Benchmark 3 complete with duration 12.768
Running py-spy profiler...
Benchmark 4 complete with duration 12.963
Running py-spy profiler...
Benchmark 5 complete with duration 12.756
Running py-spy profiler...
Benchmark 6 complete with duration 12.811
Running py-spy profiler...
Benchmark 7 complete with duration 13.066
Running py-spy profiler...
Benchmark 8 complete with duration 12.754
Running py-spy profiler...
Benchmark 9 complete with duration 12.831
Running py-spy profiler...
Benchmark 10 complete with duration 12.91

Done with COT prompting - moving to next prompt type for project whisper with optimizer 40

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    
    # Use unfold with minimal memory allocation
    slices = x.unfold(-1, filter_width, 1)
    
    # Calculate grid size more efficiently
    grid = slices.numel() // filter_width
    
    # Pre-allocate output tensor with proper memory layout
    y = torch.empty(slices.shape[:-1], dtype=x.dtype, device=x.device, 
                    memory_format=torch.contiguous_format)
    
    # Get cached kernel to avoid recompilation
    kernel = median_kernel(filter_width)
    
    # Optimize block size calculation using bit manipulation
    stride_bits = (y.stride(-1) - 1).bit_length()
    BLOCK_SIZE = 1 << min(stride_bits, 10)  # Cap at 1024 for memory efficiency
    
    # Launch kernel with optimized parameters
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001FD7FAA6F30>(tensor([[[ 7.5251e-01,  7.5251e-01,  7.5251e-01,  ..., -6.2211e-01,\n           1.0626e-01, -6.2211e-01],\n         [ 3....7.9860e-01],\n         [ 5.1719e-01, -2.5181e-01,  3.7202e-01,  ...,  6.1450e-02,\n           6.1450e-02,  6.1450e-02]]]), tensor([[[ 0.7525,  0.7525,  0.7525,  ..., -0.6221,  0.1063, -0.6221],\n         [ 0.3905,  0.3905,  0.3905,  ...,  1.2...23, -1.3223,  ...,  0.4068, -0.1720, -1.7565],\n         [-1.7565,  0.5478,  0.6235,  ..., -0.6437, -0.4607, -0.4439]]]))
E    +    where <function allclose at 0x000001FD7FAA6F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001FD7FAA6F30>(tensor([[[[ 2.9884e-01,  1.6546e+00,  2.9884e-01,  ..., -8.0857e-01,\n            2.8098e-01, -8.0857e-01],\n          [...456e+00],\n          [ 2.1459e-01,  2.1459e-01,  2.1459e-01,  ...,  1.0968e+00,\n            1.0968e+00,  1.0968e+00]]]]), tensor([[[[ 2.9884e-01,  1.6546e+00,  2.9884e-01,  ..., -8.0857e-01,\n            2.8098e-01, -8.0857e-01],\n          [...228e+00],\n          [ 1.0140e+00,  1.0140e+00,  1.0140e+00,  ..., -1.2337e-01,\n           -5.7176e-02, -1.2337e-01]]]]))
E    +    where <function allclose at 0x000001FD7FAA6F30> = np.allclose
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 13.82s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1439 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 14.303
Running py-spy profiler...
Benchmark 2 complete with duration 13.916
Running py-spy profiler...
Benchmark 3 complete with duration 14.317
Running py-spy profiler...
Benchmark 4 complete with duration 13.871
Running py-spy profiler...
Benchmark 5 complete with duration 14.069
Running py-spy profiler...
Benchmark 6 complete with duration 13.881
Running py-spy profiler...
Benchmark 7 complete with duration 14.05
Running py-spy profiler...
Benchmark 8 complete with duration 13.948
Running py-spy profiler...
Benchmark 9 complete with duration 14.261
Running py-spy profiler...
Benchmark 10 complete with duration 14.123

Done with BASE prompting - moving to next prompt type for project whisper with optimizer 40

Done with 40 - moving to next optimizer...
GENERATED META PROMPT: 
Optimize the provided code object from the "whisper" project, ensuring to maintain its original signature and interface. The goal is to produce an optimized version that improves runtime performance while adhering to the following considerations:

1. Analyze and reduce algorithmic complexity using big O notation.
2. Select and implement efficient data structures suitable for the task.
3. Optimize loops to eliminate redundant iterations and calculations.
4. Enhance memory access patterns for better cache utilization.
5. Minimize I/O operations and system calls to reduce overhead.
6. Explore opportunities for parallel processing and multi-threading.
7. Eliminate any redundant computations for increased efficiency.

Return only the optimized version of the code object, focusing on advanced strategies such as vectorization and system-level optimizations, while utilizing GPT-4-O's ability to internally verify assumptions and performance metrics.
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 101
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](cost, trace, x_skew, x_skew.stride(0), cost.stride(0), trace.stride(0), N, M, BLOCK_SIZE=BLOCK_SIZE)

    trace = trace.T[:N+1].cpu().numpy()
    return backtrace(trace)
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,28) (2,39)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,44) (2,49)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1587) (2,1708)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,392) (2,424)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.59s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1569 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(d_out, d_in, stride, out_stride, block_size, filter_width):
    shared_memory = cuda.shared.array(shape=0, dtype=numba.float32)
    idx = cuda.grid(1)
    offset = idx * stride
    if idx < d_out.size:
        temp = d_in[offset:offset + filter_width]
        cuda.syncthreads()
        shared_memory[cuda.threadIdx.x] = cuda.sort(temp)[len(temp) // 2]
        cuda.syncthreads()
        d_out[idx] = shared_memory[cuda.threadIdx.x]


def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid_size = slices.shape.numel() // slices.shape[-2]
    y = torch.empty(slices.shape[:-1], device=x.device)

    block_size = min(grid_size, 512)  # Optimal block size
    threads_per_block = block_size

    x_flat = x.reshape(-1, x.shape[-1])
    y_flat = y.view(-1)

    median_kernel[grid_size // threads_per_block, threads_per_block](y_flat, x_flat, x_flat.stride(-1), y_flat.stride(0), block_size, filter_width)

    return y.view(*x.shape[:-1])
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size // threads_per_block, threads_per_block](y_flat, x_flat, x_flat.stride(-1), y_flat.stride(0), block_size, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size // threads_per_block, threads_per_block](y_flat, x_flat, x_flat.stride(-1), y_flat.stride(0), block_size, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size // threads_per_block, threads_per_block](y_flat, x_flat, x_flat.stride(-1), y_flat.stride(0), block_size, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size // threads_per_block, threads_per_block](y_flat, x_flat, x_flat.stride(-1), y_flat.stride(0), block_size, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
tests/test_timing.py::test_median_filter_equivalence[shape2]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape3]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 101 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 3 warnings in 13.35s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1339 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np

@torch.jit.script
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1).contiguous()
    grid = slices.numel() // filter_width
    y = torch.empty((*slices.shape[:-1],), device=x.device, dtype=x.dtype)
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    def median_kernel(output, input, stride_in, stride_out, block_size):
        for idx in range(grid):
            start = (idx * stride_in) // block_size
            stop = start + filter_width
            output[idx] = input[start:stop].median()

    median_kernel(y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE)
    return y

===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF....FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:103: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:103: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:103: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:103: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:31: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:31: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:31: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:31: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
E   
E       def median_kernel(output, input, stride_in, stride_out, block_size):
E       ~~~ <--- HERE
E           for idx in range(grid):
E               start = (idx * stride_in) // block_size
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================= 10 failed, 16 passed in 12.94s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1382 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(d_out, d_in, stride_x, stride_y, filter_width):
    idx = cuda.grid(1)
    if idx < d_out.size:
        start = idx * stride_y
        end = start + filter_width
        d_out[idx] = gpu_median(d_in[start:end])

@cuda.jit(device=True)
def gpu_median(data):
    sorted_data = cuda.local.array((1 << 10,), dtype=float32)  # adjust size appropriately
    n = data.shape[0]
    for i in range(n):
        sorted_data[i] = data[i]

    for i in range(n):
        for j in range(i + 1, n):
            if sorted_data[i] > sorted_data[j]:
                sorted_data[i], sorted_data[j] = sorted_data[j], sorted_data[i]
    
    if n % 2 == 0:
        return (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2.0
    else:
        return sorted_data[n//2]

def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = np.prod(slices.shape[:-2])
    y = torch.empty_like(slices[..., 0])
    BLOCK_SIZE = 256
    threads_per_block = (BLOCK_SIZE,)
    blocks_per_grid = (grid + (BLOCK_SIZE - 1)) // BLOCK_SIZE
    if slices.numel() > 0:
        median_kernel[blocks_per_grid, threads_per_block](y, slices, slices.stride(-2), y.stride(-2), filter_width)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:143: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, slices, slices.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:143: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, slices, slices.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:143: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, slices, slices.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:143: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, slices, slices.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape3]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 68 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.46s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1449 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
from numba import cuda
import torch
import numpy as np

@cuda.jit
def median_kernel(y, x, x_stride, y_stride, filter_width):
    idx = cuda.grid(1)
    if idx < y.size:
        i = idx // y_stride
        j = idx % y_stride
        start = i * x_stride + j
        window = x[start:start + filter_width]
        window.sort()
        y[idx] = window[len(window) // 2]

def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices_shape = x.size()[:-1] + (x.size(-1) - filter_width + 1, filter_width)
    slices = x.contiguous().unfold(-1, filter_width, 1)
    y_shape = slices_shape[:-1]
    y = torch.empty(y_shape, dtype=x.dtype, device='cuda')
    
    grid_size = slices.numel() // slices.size(-1)
    median_kernel[grid_size, 256](y, x, x.stride(-2), y.stride(-2), filter_width)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[grid_size, 256](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[grid_size, 256](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[grid_size, 256](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[grid_size, 256](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 10 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape1]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 15 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.35s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1372 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(flatten_y, x, stride_x, stride_y):
    idx = cuda.grid(1)
    if idx < flatten_y.size:
        window = x[idx * stride_x: idx * stride_x + stride_x]
        flatten_y[idx] = torch.median(window)

def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = np.prod(slices.shape[:-2])

    y = torch.empty_like(slices[..., 0])
    flatten_y = y.contiguous().view(-1)

    BLOCK_SIZE = 256  # commonly 256 is a good block size
    num_blocks = (grid + BLOCK_SIZE - 1) // BLOCK_SIZE

    median_kernel[num_blocks, BLOCK_SIZE](flatten_y, x, filter_width, y.size(-1))

    return y.reshape(slices.shape[:-1])
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[num_blocks, BLOCK_SIZE](flatten_y, x, filter_width, y.size(-1))
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[num_blocks, BLOCK_SIZE](flatten_y, x, filter_width, y.size(-1))
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[num_blocks, BLOCK_SIZE](flatten_y, x, filter_width, y.size(-1))
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[num_blocks, BLOCK_SIZE](flatten_y, x, filter_width, y.size(-1))
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape3]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 68 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.45s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1419 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(y, x, x_stride, y_stride, filter_width):
    idx = cuda.grid(1)
    if idx < x.size // x_stride:
        start = (idx // y_stride) * x_stride + (idx % y_stride)
        values = [x[start + i] for i in range(filter_width)]
        y[idx] = np.median(values)


def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1).contiguous()
    grid = np.prod(slices.shape[:-2])

    y = torch.empty_like(slices[..., 0])

    threads_per_block = 128
    blocks_per_grid = (grid + threads_per_block - 1) // threads_per_block

    median_kernel[blocks_per_grid, threads_per_block](y.flatten(), x.flatten(), x.stride(-2), y.stride(-2), filter_width)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y.flatten(), x.flatten(), x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y.flatten(), x.flatten(), x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y.flatten(), x.flatten(), x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:128: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y.flatten(), x.flatten(), x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 1 warning in 13.30s ===================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1406 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

6 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(y, x, stride_x, stride_y, filter_width):
    idx = cuda.grid(1)
    if idx < y.size:
        local_slice = cuda.shared.array(shape=(1,), dtype=x.dtype)
        start_idx = (idx // stride_y) * stride_x + (idx % stride_y)
        for i in range(filter_width):
            local_slice[i] = x[start_idx + i]
        cuda.syncthreads()
        # Sort the local_slice
        for i in range(filter_width):
            for j in range(i + 1, filter_width):
                if local_slice[i] > local_slice[j]:
                    local_slice[i], local_slice[j] = local_slice[j], local_slice[i]
        # Assign median value
        y[idx] = local_slice[filter_width // 2]


def median_filter_cuda(x: torch.Tensor, filter_width: int) -> torch.Tensor:
    x_unfolded = x.contiguous().unfold(-1, filter_width, 1)
    y = torch.empty(*x_unfolded.shape[:-1], dtype=x.dtype, device=x.device)
    grid_size = x_unfolded.numel() // filter_width
    block_size = 256  # Chosen based on hardware capability

    median_kernel[grid_size, block_size](y, x, filter_width)
    return y

===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size, block_size](y, x, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size, block_size](y, x, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size, block_size](y, x, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:134: in median_filter_cuda
    median_kernel[grid_size, block_size](y, x, filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 10 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape1]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 15 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.36s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1444 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

7 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
from numba import cuda

@cuda.jit
def median_kernel(y, x, stride_x, stride_y, filter_width):
    i = cuda.grid(1)
    if i < y.size:
        start = i * stride_y
        end = start + stride_x * filter_width
        window = x[start:end:stride_x]
        y[i] = torch.median(window)

def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.contiguous().unfold(-1, filter_width, 1)
    output_shape = slices.shape[:-1]
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device)

    grid = slices.numel() // slices.shape[-2]
    threads_per_block = 256
    blocks_per_grid = (grid + threads_per_block - 1) // threads_per_block

    median_kernel[blocks_per_grid, threads_per_block](y, x, slices.shape[-1], y.stride(-1), filter_width)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, slices.shape[-1], y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, slices.shape[-1], y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, slices.shape[-1], y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, slices.shape[-1], y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 1 warning in 13.23s ===================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1382 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

8 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    import torch
    import numpy as np
    from numba import cuda
    
    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = slices.shape[:-2].numel()
    
    @cuda.jit
    def median_kernel(y, x, stride_x, stride_y, block_size):
        idx = cuda.grid(1)
        if idx < y.size:
            start = idx // stride_y * stride_y * block_size + (idx % stride_y) * stride_x
            window = x[start:start + filter_width]
            y[idx] = np.median(window)

    y = torch.empty(slices.shape[:-1], device='cuda')
    BLOCK_SIZE = min(1024, 1 << (y.stride(-2) - 1).bit_length())
    threads_per_block = BLOCK_SIZE
    blocks_per_grid = (grid + (threads_per_block - 1)) // threads_per_block
    
    median_kernel[blocks_per_grid, threads_per_block](y, x, x.stride(-2), y.stride(-2), filter_width)
    
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:127: in median_filter_cuda
    median_kernel[blocks_per_grid, threads_per_block](y, x, x.stride(-2), y.stride(-2), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
tests/test_timing.py::test_median_filter_equivalence[shape1]
tests/test_timing.py::test_median_filter_equivalence[shape2]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape3]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 34 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 4 warnings in 13.36s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1356 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

9 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(y, x, stride_x, stride_y, filter_width):
    idx = cuda.grid(1)
    if idx >= y.size:  # safeguard index
        return
    start = idx * stride_y
    end = start + filter_width
    segment = x[start:end]
    segment_sorted = cuda.local.array((32,), dtype=x.dtype)  # assuming max filter_width
    segment_sorted[:filter_width] = segment[:filter_width]  # copy to local array
    segment_sorted[:filter_width].sort()  # in-place sort for efficiency, safe in CUDA
    median_idx = filter_width // 2
    y[idx] = segment_sorted[median_idx]


def median_filter_cuda(x: torch.Tensor, filter_width: int) -> torch.Tensor:
    slices = x.contiguous().unfold(-1, filter_width, 1)
    output_shape = slices.shape[:-1]
    grid = np.prod(output_shape)
    y = torch.empty(output_shape, dtype=x.dtype, device=x.device)
    BLOCK_SIZE = 1024  # typical block size for CUDA hardware
    median_kernel[(grid // BLOCK_SIZE + 1,), (BLOCK_SIZE,)](y.view(-1), x, x.stride(-1), y.stride(-1), filter_width)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:131: in median_filter_cuda
    median_kernel[(grid // BLOCK_SIZE + 1,), (BLOCK_SIZE,)](y.view(-1), x, x.stride(-1), y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:131: in median_filter_cuda
    median_kernel[(grid // BLOCK_SIZE + 1,), (BLOCK_SIZE,)](y.view(-1), x, x.stride(-1), y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:131: in median_filter_cuda
    median_kernel[(grid // BLOCK_SIZE + 1,), (BLOCK_SIZE,)](y.view(-1), x, x.stride(-1), y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:32: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:131: in median_filter_cuda
    median_kernel[(grid // BLOCK_SIZE + 1,), (BLOCK_SIZE,)](y.view(-1), x, x.stride(-1), y.stride(-1), filter_width)
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape2]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.43s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1419 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

10 failed optimizations : regenerating attempt...
OptimizationError
Error optimizing whisper with 4o: Failed to optimize code object
def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """Apply a median filter of given width along the last dimension of x"""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y

with optimizer '4o' after 10 attempts
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 17.817
Running py-spy profiler...
Benchmark 2 complete with duration 18.049
Running py-spy profiler...
Benchmark 3 complete with duration 17.614
Running py-spy profiler...
Benchmark 4 complete with duration 17.564
Running py-spy profiler...
Benchmark 5 complete with duration 17.729
Running py-spy profiler...
Benchmark 6 complete with duration 17.993
Running py-spy profiler...
Benchmark 7 complete with duration 18.124
Running py-spy profiler...
Benchmark 8 complete with duration 18.053
Running py-spy profiler...
Benchmark 9 complete with duration 17.9
Running py-spy profiler...
Benchmark 10 complete with duration 17.767

Done with MP prompting - moving to next prompt type for project whisper with optimizer 4o

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 84
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"
    x_skew = F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)
    dtw_kernel[(1,)](cost, trace, x_skew, x_skew.stride(0), cost.stride(0), trace.stride(0), N, M, BLOCK_SIZE=BLOCK_SIZE)
    return backtrace(trace.T.cpu().numpy()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1])
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:94: in dtw_cuda
    return backtrace(trace.T.cpu().numpy()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: cannot reshape array of size 384 into shape (11,33)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:94: in dtw_cuda
    return backtrace(trace.T.cpu().numpy()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: cannot reshape array of size 1700 into shape (33,51)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:94: in dtw_cuda
    return backtrace(trace.T.cpu().numpy()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: cannot reshape array of size 203125 into shape (124,1626)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:94: in dtw_cuda
    return backtrace(trace.T.cpu().numpy()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: cannot reshape array of size 100300 into shape (235,426)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.66s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1474 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 84
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = F.pad(x, (0, M + 1), value=np.inf)[:M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace.T[:(M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, : N + 1]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:91: in dtw_cuda
    x_skew = F.pad(x, (0, M + 1), value=np.inf)[:M * (N + M)].reshape(M, N + M).T.contiguous()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[10, 30]' is invalid for input of size 310
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:91: in dtw_cuda
    x_skew = F.pad(x, (0, M + 1), value=np.inf)[:M * (N + M)].reshape(M, N + M).T.contiguous()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[32, 48]' is invalid for input of size 1568
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:91: in dtw_cuda
    x_skew = F.pad(x, (0, M + 1), value=np.inf)[:M * (N + M)].reshape(M, N + M).T.contiguous()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[123, 1623]' is invalid for input of size 199752
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:91: in dtw_cuda
    x_skew = F.pad(x, (0, M + 1), value=np.inf)[:M * (N + M)].reshape(M, N + M).T.contiguous()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[234, 423]' is invalid for input of size 99216
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.65s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1449 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 84
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"
    x_skew = F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)
    dtw_kernel[(1,)](cost, trace, x_skew, x_skew.stride(0), cost.stride(0), trace.stride(0), N, M, BLOCK_SIZE=BLOCK_SIZE)
    trace = trace.T[:, : N + 1]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
E   assert False
E    +  where False = <function allclose at 0x000002CD57E46F30>(array([[ 0,  1,  2,  3,  3,  3,  3,  3,  3,  3,  3,  4,  5,  6,  6,  6,\n         7,  7,  7,  7,  7,  7,  7,  8,  8,  9...,  0,  1,  1,  2,  3,  4,  5,  6,  7,  8,  8,  8,  8,  9, 10,\n        10, 11, 12, 13, 14, 15, 16, 16, 17, 17, 18, 19]]), array([[-1,  0,  0,  0,  1,  2,  2,  3,  3,  3,  3,  3,  3,  4,  4,  4,\n         5,  5,  6,  6,  7,  7,  8,  8,  9,  9...,  1,  2,  3,  3,  4,  5,  5,  6,  7,  8,  9, 10, 10, 11, 12,\n        12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 18, 19]]))
E    +    where <function allclose at 0x000002CD57E46F30> = np.allclose
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,45) (2,33)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1580) (2,1583)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,389) (2,235)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.07s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1540 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 84
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"
    x_skew = F.pad(x, (0, M + 1), value=np.inf).view(-1)[:M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)
    dtw_kernel[(1,)](cost, trace, x_skew, x_skew.stride(0), cost.stride(0), trace.stride(0), N, M, BLOCK_SIZE=BLOCK_SIZE)
    trace = trace[:M + 1, :N + 1]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,25) (2,11)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,45) (2,47)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1580) (2,124)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,396) (2,311)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.17s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1440 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 84
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"
    x_skew = F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M).T.contiguous()
    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)
    dtw_kernel[(1,)](cost, trace, x_skew, x_skew.stride(0), cost.stride(0), trace.stride(0), N, M, BLOCK_SIZE=BLOCK_SIZE)
    trace = trace.T[:M + 1, :N + 1]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,26) (2,27)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,43) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
E   assert False
E    +  where False = <function allclose at 0x000001DCC9D16F30>(array([[   0,    0,    0, ...,  121,  122,  122],\n       [   0,    1,    2, ..., 1498, 1498, 1499]], shape=(2, 1584)), array([[  -1,    0,    0, ...,  121,  122,  122],\n       [   0,    1,    2, ..., 1497, 1498, 1499]], shape=(2, 1584)))
E    +    where <function allclose at 0x000001DCC9D16F30> = np.allclose
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,383) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.27s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1529 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    y = torch.empty_like(slices[..., 0])

    grid = slices.shape[:-2].numel()
    kernel = median_kernel(filter_width)
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001FEFF4D6F30>(tensor([[[ 2.6672e-01,  2.6672e-01,  2.6672e-01,  ..., -2.6389e-01,\n          -4.6714e-01, -1.0732e+00],\n         [ 3....9.1908e-01],\n         [ 7.8148e-02, -4.2469e-01,  3.0320e-02,  ..., -5.0264e-01,\n          -4.5573e-01, -5.0264e-01]]]), tensor([[[ 0.2667,  0.2667,  0.2667,  ..., -0.2639, -0.4671, -1.0732],\n         [ 0.2667,  0.2667, -0.0083,  ..., -0.4...60, -0.9260,  ...,  0.1592, -0.8249, -0.7242],\n         [-0.9260, -0.9260, -0.0843,  ..., -0.8249, -0.7242, -0.7242]]]))
E    +    where <function allclose at 0x000001FEFF4D6F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001FEFF4D6F30>(tensor([[[[ 7.8880e-01,  3.2783e-01,  6.2674e-01,  ...,  9.4396e-02,\n            9.4396e-02, -1.2349e+00],\n          [...450e-01],\n          [ 2.4349e+00, -1.5351e-01, -2.3581e-03,  ..., -1.6907e+00,\n           -4.6978e-02, -1.6907e+00]]]]), tensor([[[[ 7.8880e-01,  3.2783e-01,  6.2674e-01,  ...,  9.4396e-02,\n            9.4396e-02, -1.2349e+00],\n          [...832e-01],\n          [-2.0133e-01, -2.0133e-01, -2.8930e-02,  ...,  3.7832e-01,\n            3.7832e-01, -3.1562e-01]]]]))
E    +    where <function allclose at 0x000001FEFF4D6F30> = np.allclose
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 12.60s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1261 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = slices.numel() // slices.shape[-1]

    kernel = median_kernel(filter_width)
    y = torch.empty_like(slices[..., 0])

    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[(grid,)](y, slices, slices.stride(-2), slices.stride(-1), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000012125FC6F30>(tensor([[[ 1.0947,  0.3632,  0.2670,  ...,  0.1972,  0.1174, -0.6005],\n         [-0.0686, -0.0686, -0.0689,  ...,  0.2...07, -0.5289,  ...,  0.6143,  0.2799, -1.0298],\n         [-0.5558, -0.2839, -0.5558,  ..., -0.9808,  0.3961, -0.9808]]]), tensor([[[ 1.0947,  0.3632,  0.2670,  ...,  0.1972,  0.1174, -0.6005],\n         [-0.0686, -0.0686, -0.0686,  ..., -0.0...77, -0.3870,  ..., -0.5203,  0.5171,  0.5171],\n         [ 0.3599,  0.2704,  0.1638,  ..., -1.3914, -0.2594, -0.1819]]]))
E    +    where <function allclose at 0x0000012125FC6F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000012125FC6F30>(tensor([[[[ 2.5937e-01,  5.7051e-01,  2.5937e-01,  ..., -1.1848e-01,\n           -6.8342e-01, -6.8342e-01],\n          [...095e-01],\n          [-1.6710e-01,  4.9124e-02, -1.6710e-01,  ..., -2.8252e-01,\n            1.6192e+00, -2.8252e-01]]]]), tensor([[[[ 0.2594,  0.5705,  0.2594,  ..., -0.1185, -0.6834, -0.6834],\n          [-1.5464, -0.6834, -1.9825,  ..., -0...,  1.6246,  ...,  1.0643,  0.8514,  0.6142],\n          [ 0.6142,  0.6142, -0.2394,  ...,  0.3063,  0.8314,  0.2891]]]]))
E    +    where <function allclose at 0x0000012125FC6F30> = np.allclose
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 13.13s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1348 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.shape[0]
    kernel = median_kernel(filter_width)
    y = torch.empty(slices.shape[:-1], device=x.device)
    BLOCK_SIZE = min(1024, slices.shape[-1])
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, slices, slices.stride(-2), y.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.69s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1396 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.view(-1, *slices.shape[-2:]).shape[0]

    kernel = median_kernel(filter_width)
    y = torch.empty((grid, *slices.shape[-1:]), device=x.device)

    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y

# Note: median_kernel definition is assumed to be unchanged from original.
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:749: in run
    grid_size = len(grid)
                ^^^^^^^^^
E   TypeError: object of type 'int' has no len()
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:749: in run
    grid_size = len(grid)
                ^^^^^^^^^
E   TypeError: object of type 'int' has no len()
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:749: in run
    grid_size = len(grid)
                ^^^^^^^^^
E   TypeError: object of type 'int' has no len()
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:749: in run
    grid_size = len(grid)
                ^^^^^^^^^
E   TypeError: object of type 'int' has no len()
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.34s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1347 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.shape[:-1]
    kernel = median_kernel(filter_width)
    y = torch.empty(slices[..., 0].shape, device=x.device)
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[grid](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000002218C136F30>(tensor([[[ 1.8512,  1.5633,  1.5633,  ..., -0.1309, -0.1309, -0.7775],\n         [ 0.4169,  0.4169,  0.4169,  ..., -0.6...04, -0.0504,  ...,  0.5711,  0.5757,  0.5711],\n         [-0.8012,  0.0686, -0.5032,  ...,  0.7053,  1.1335,  0.4150]]]), tensor([[[ 1.8512,  1.5633,  1.5633,  ..., -0.1309, -0.1309, -0.7775],\n         [ 0.4169,  0.4169,  0.4169,  ..., -0.6...04, -2.4976,  ...,  0.5757,  0.5711,  1.9413],\n         [ 0.0686, -0.8012,  0.6125,  ...,  1.9591,  0.4150,  1.1335]]]))
E    +    where <function allclose at 0x000002218C136F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000002218C136F30>(tensor([[[[-1.2596e+00,  1.6683e-01, -1.1363e+00,  ..., -1.0235e+00,\n           -9.6936e-01, -1.0235e+00],\n          [...744e-01],\n          [-1.6575e+00, -4.1545e-01, -1.6358e+00,  ..., -9.3515e-02,\n           -5.7924e-01, -1.1603e+00]]]]), tensor([[[[-1.2596e+00,  1.6683e-01, -1.1363e+00,  ..., -1.0235e+00,\n           -9.6936e-01, -1.0235e+00],\n          [...318e-01],\n          [ 1.7618e-01, -1.6575e+00, -4.1545e-01,  ..., -9.3515e-02,\n           -1.1603e+00, -5.7924e-01]]]]))
E    +    where <function allclose at 0x000002218C136F30> = np.allclose
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 12.19s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1294 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.numel() // slices.size(-2)
    kernel = median_kernel(filter_width)
    y = torch.empty(slices.size()[:-1], dtype=slices.dtype, device=slices.device)
    BLOCK_SIZE = 1 << (grid - 1).bit_length()
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001D634936F30>(tensor([[[ 1.0713,  1.0713,  1.7371,  1.7371, -0.0517, -0.6279, -0.6279,\n          -0.6739, -0.6739, -0.6739]]]), tensor([[[ 1.0713,  1.0713,  1.7371,  1.7371, -0.0517, -0.6279, -0.7757,\n           0.1253, -0.6739, -1.0920]]]))
E    +    where <function allclose at 0x000001D634936F30> = np.allclose
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001D634936F30>(tensor([[[[ 0.7566, -0.1535, -0.1535, -1.1139, -1.1139, -0.1658,  0.0324,\n           -0.1658, -0.4565, -0.4565,  0.2248,  0.2248,  0.2248,  0.4213,\n            0.5933]]]]), tensor([[[[ 0.7566, -0.1535, -0.1535, -1.1139, -1.1139,  0.0585, -0.1658,\n            0.0324, -0.4565, -0.9595,  0.6592,  0.2248, -1.6387,  0.5933,\n            0.4213]]]]))
E    +    where <function allclose at 0x000001D634936F30> = np.allclose
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x000001D634936F30>(tensor([[[-0.3959,  0.3639, -0.2563,  ..., -0.9585, -0.6091, -1.7410],\n         [ 0.5938,  1.1956,  0.5938,  ...,  0.4...18, -0.7794,  ..., -0.2918, -0.2918, -0.2918],\n         [-1.4260, -1.4260, -1.7061,  ...,  1.0506,  0.8097,  1.0506]]]), tensor([[[-0.3959,  0.3639, -0.2563,  ..., -0.6091, -1.7410,  0.4343],\n         [ 0.5938,  1.1956,  0.5938,  ...,  0.2...18, -0.7794,  ..., -1.7644, -0.2918, -0.0983],\n         [-1.4260, -1.4260, -1.7061,  ...,  0.8097,  1.0506, -1.1710]]]))
E    +    where <function allclose at 0x000001D634936F30> = np.allclose
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:120: in load_model
    checkpoint = torch.load(fp, map_location=device, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed in 157.13s (0:02:37) ===================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 15349 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

6 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    y = torch.empty_like(slices[..., 0])
    grid = slices.shape[:-2].numel()
    kernel = median_kernel(filter_width)
    kernel[(grid,)](y, slices, BLOCK_SIZE=1 << slices.stride(-2).bit_length())
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, BLOCK_SIZE=1 << slices.stride(-2).bit_length())
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() missing 2 required positional arguments: 'x_stride' and 'y_stride'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, BLOCK_SIZE=1 << slices.stride(-2).bit_length())
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() missing 2 required positional arguments: 'x_stride' and 'y_stride'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, BLOCK_SIZE=1 << slices.stride(-2).bit_length())
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() missing 2 required positional arguments: 'x_stride' and 'y_stride'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, BLOCK_SIZE=1 << slices.stride(-2).bit_length())
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:723: in run
    bound_args, specialization, options = binder(*args, **kwargs)
                                          ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: dynamic_func() missing 2 required positional arguments: 'x_stride' and 'y_stride'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.31s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1325 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

7 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.shape[0] * slices.shape[1]
    kernel = median_kernel(filter_width)
    y = torch.empty_like(slices[..., 0])
    BLOCK_SIZE = min(1024, slices.size(-1))
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:27: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:123: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.86s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1336 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

8 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 13.613
Running py-spy profiler...
Benchmark 2 complete with duration 13.601
Running py-spy profiler...
Benchmark 3 complete with duration 13.536
Running py-spy profiler...
Benchmark 4 complete with duration 13.517
Running py-spy profiler...
Benchmark 5 complete with duration 13.634
Running py-spy profiler...
Benchmark 6 complete with duration 13.598
Running py-spy profiler...
Benchmark 7 complete with duration 13.558
Running py-spy profiler...
Benchmark 8 complete with duration 13.497
Running py-spy profiler...
Benchmark 9 complete with duration 13.521
Running py-spy profiler...
Benchmark 10 complete with duration 13.578

Done with FS prompting - moving to next prompt type for project whisper with optimizer 4o

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 100
import torch
import numpy as np
import torch.nn.functional as F


def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = F.pad(x, (0, M + 1), value=np.inf)
    x_skew = x_skew[:, : M * (N + M)].reshape(M, N + M).T

    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=x.device)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace.T[: M + 1, : M + N + 3]

    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:113: in dtw_cuda
    x_skew = x_skew[:, : M * (N + M)].reshape(M, N + M).T
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[10, 30]' is invalid for input of size 310
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:113: in dtw_cuda
    x_skew = x_skew[:, : M * (N + M)].reshape(M, N + M).T
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[32, 48]' is invalid for input of size 1568
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:113: in dtw_cuda
    x_skew = x_skew[:, : M * (N + M)].reshape(M, N + M).T
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[123, 1623]' is invalid for input of size 199752
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:113: in dtw_cuda
    x_skew = x_skew[:, : M * (N + M)].reshape(M, N + M).T
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[234, 423]' is invalid for input of size 99216
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.05s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1431 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
from numba import cuda

def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.contiguous().unfold(-1, filter_width, 1)
    y = torch.empty_like(slices[..., 0])

    grid_size = (slices.shape[0] + 31) // 32, slices.shape[1]
    block_size = 32, filter_width

    kernel[grid_size, block_size](y, x, filter_width, slices.shape[-1])

    return y

@cuda.jit
def kernel(y, x, filter_width, len_last_dim):
    i, j = cuda.grid(2)
    if i < x.shape[0] and j < len_last_dim:
        start = cuda.local.array(32, dtype=numba.float32)
        for k in range(filter_width):
            start[k] = x[i, j + k]
        start[:filter_width].sort()
        y[i, j] = start[filter_width // 2]
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:36: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:116: in median_filter_cuda
    kernel[grid_size, block_size](y, x, filter_width, slices.shape[-1])
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:36: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:116: in median_filter_cuda
    kernel[grid_size, block_size](y, x, filter_width, slices.shape[-1])
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:36: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:116: in median_filter_cuda
    kernel[grid_size, block_size](y, x, filter_width, slices.shape[-1])
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:36: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:116: in median_filter_cuda
    kernel[grid_size, block_size](y, x, filter_width, slices.shape[-1])
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:122: in load_model
    model = Whisper(dims)
            ^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\modules\module.py:494: in __init__
    raise TypeError(
E   TypeError: Whisper.__init__() takes 1 positional argument but 2 were given
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape2]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 5 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape3]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 12 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 3 warnings in 12.35s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1300 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 13.13
Running py-spy profiler...
Benchmark 2 complete with duration 12.977
Running py-spy profiler...
Benchmark 3 complete with duration 12.999
Running py-spy profiler...
Benchmark 4 complete with duration 12.866
Running py-spy profiler...
Benchmark 5 complete with duration 13.834
Running py-spy profiler...
Benchmark 6 complete with duration 12.986
Running py-spy profiler...
Benchmark 7 complete with duration 13.065
Running py-spy profiler...
Benchmark 8 complete with duration 13.052
Running py-spy profiler...
Benchmark 9 complete with duration 13.491
Running py-spy profiler...
Benchmark 10 complete with duration 13.592

Done with COT prompting - moving to next prompt type for project whisper with optimizer 4o

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 97
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    
    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = F.pad(x, (0, M + 1), value=np.inf)
    x_skew = x_skew[:, :M*(N+M)].transpose(0, 1).contiguous()

    cost = torch.full((N + M + 2, M + 2), np.inf, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace.transpose(0, 1)[:M + 1, :M + N + 3]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,25) (2,31)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,44) (2,49)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1586) (2,1624)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,392) (2,424)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.23s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1542 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.numel() // filter_width
    y = torch.empty(slices.shape[:-1], device=x.device)

    BLOCK_SIZE = 1 << (y.stride(-1) - 1).bit_length()
    kernel[(grid, BLOCK_SIZE)](y, x, x.stride(-1), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, x.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, x.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, x.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:112: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, x.stride(-1), BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.60s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1370 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid_size = slices.numel() // slices.shape[-1]

    kernel = median_kernel(filter_width)
    y = torch.empty_like(slices[..., 0])

    BLOCK_SIZE = min(1024, grid_size)
    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:114: in median_filter_cuda
    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                 ^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:127: in load_model
    checkpoint = torch.load(fp, map_location=device, **load_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.38s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1341 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.shape[:-2].numel()
    y = torch.empty(slices.size(0), slices.size(1), device=slices.device, dtype=slices.dtype)
    kernel = median_kernel(filter_width)
    BLOCK_SIZE = max(1, 1 << (y.stride(-2) - 1).bit_length())

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000017B65C26F30>(tensor([[[ 0.9892,  0.0901,  0.0901,  0.0901, -0.9330,  0.6281, -0.7489,\n           0.1376,  0.1376,  0.1376]]]), tensor([[0.9892]]))
E    +    where <function allclose at 0x0000017B65C26F30> = np.allclose
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000017B65C26F30>(tensor([[[[ 0.9896,  1.0455,  0.9896,  0.2008, -0.1227, -0.2571, -0.2571,\n           -0.2571,  2.1970,  2.1970,  1.7727,  0.0975,  0.4342, -0.9211,\n            0.4342]]]]), tensor([[0.9896]]))
E    +    where <function allclose at 0x0000017B65C26F30> = np.allclose
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (4,5,345) (4,5)
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (6,12,240,512) (6,12)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.59s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1386 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
from numba import cuda
import numpy as np
import torch

@cuda.jit
def median_kernel(y, slices, stride_x, stride_y, filter_width):
    # Calculate flat index of grid and block
    idx = cuda.grid(1)
    
    grid_stride = cuda.gridsize(1)
    while idx < slices.size(0):
        # Access slice for the current index and apply median filter
        slice_values = slices[idx]
        # Use shared memory for the block
        shared_data = cuda.shared.array(shape=(256,), dtype=numba.float32)
        tid = cuda.threadIdx.x
        
        # Load slice data into shared memory
        if tid < filter_width:
            shared_data[tid] = slice_values[tid]

        # Ensure all writes to shared memory are completed
        cuda.syncthreads()

        # Perform median filtering, assuming small filter width for simplicity
        filtered_value = np.median(shared_data[:filter_width])
        y[idx] = filtered_value

        # Move to the next index for processing
        idx += grid_stride


def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    if x.is_cuda:
        # Expect input tensor to be on GPU
        device = x.device
    else:
        raise ValueError("Input tensor must be on GPU")

    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = slices.size(0) * slices.size(1)

    # Prepare output tensor
    y = torch.empty(slices.size(0), slices.size(1), device=device)

    # Calculate necessary block size for kernel
    grid_size = (grid + 255) // 256

    # Use CUDA kernel for median filtering
    median_kernel[grid_size, 256](
        y, slices, slices.stride(-2), y.stride(-1), filter_width
    )

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:156: in median_filter_cuda
    median_kernel[grid_size, 256](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:156: in median_filter_cuda
    median_kernel[grid_size, 256](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:156: in median_filter_cuda
    median_kernel[grid_size, 256](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:156: in median_filter_cuda
    median_kernel[grid_size, 256](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 1 warning in 13.32s ===================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1413 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np

@torch.jit.script
def median_filter_cuda(x: torch.Tensor, filter_width: int) -> torch.Tensor:
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.numel() // filter_width
    y = torch.empty(slices.size()[:-1], device=x.device, dtype=x.dtype)
    BLOCK_SIZE = 32
    
    def median_kernel(y, slices, stride, block_size):
        for grid_index in range(grid):
            start_index = grid_index * stride
            aperture = slices[start_index:start_index + filter_width]
            y[grid_index] = torch.median(aperture).item()
    
    median_kernel(y, slices, slices.stride(0), BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF....FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:99: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:99: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:99: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:62: in test_dtw_cuda_equivalence
    trace_cuda = dtw_cuda(x_cuda)
                 ^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:99: in dtw_cuda
    from .triton_ops import dtw_kernel
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:33: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:33: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:33: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:33: in median_filter
    from .triton_ops import median_filter_cuda
projects\whisper\whisper\triton_ops.py:109: in <module>
    @torch.jit.script
     ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\_script.py:1464: in script
    ret = _script_impl(
venv\Lib\site-packages\torch\jit\_script.py:1232: in _script_impl
    ast = get_jit_def(obj, obj.__name__)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:377: in get_jit_def
    return build_def(
venv\Lib\site-packages\torch\jit\frontend.py:442: in build_def
    return Def(Ident(r, def_name), decl, build_stmts(ctx, body))
                                         ^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:191: in build_stmts
    stmts = [build_stmt(ctx, s) for s in stmts]
             ^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\jit\frontend.py:410: in __call__
    raise UnsupportedNodeError(ctx, node)
E   torch.jit.frontend.UnsupportedNodeError: function definitions aren't supported:
E     File "C:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper\whisper\triton_ops.py", line 116
E       BLOCK_SIZE = 32
E       
E       def median_kernel(y, slices, stride, block_size):
E       ~~~ <--- HERE
E           for grid_index in range(grid):
E               start_index = grid_index * stride
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================= 10 failed, 16 passed in 12.71s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1355 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):

    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = slices.shape[0] * slices.shape[1] * slices.shape[2]

    y = torch.empty_like(slices[..., 0])

    BLOCK_SIZE = 32
  
    kernel[(grid, BLOCK_SIZE)](y, x, slices.stride(-2), y.stride(-2))

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, slices.stride(-2), y.stride(-2))
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, slices.stride(-2), y.stride(-2))
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, slices.stride(-2), y.stride(-2))
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid, BLOCK_SIZE)](y, x, slices.stride(-2), y.stride(-2))
    ^^^^^^
E   NameError: name 'kernel' is not defined
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.60s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1363 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

6 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    y = torch.empty_like(slices[..., 0])
    grid_size = slices.numel() // slices.size(-1)

    kernel = median_kernel(filter_width)

    BLOCK_SIZE = (grid_size + 255) // 256  # Optimize block size calculation

    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000024AC9416F30>(tensor([[[-0.4949, -0.4949, -0.4949, -1.1059,  0.0633, -0.0869, -0.0869,\n          -0.8876, -0.8876, -0.8876]]]), tensor([[[-0.4949, -0.4949, -1.1105,  0.8706, -1.1059,  0.0633, -0.0869,\n          -1.8893, -0.8876,  0.1917]]]))
E    +    where <function allclose at 0x0000024AC9416F30> = np.allclose
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:96: in test_median_filter_equivalence
    assert np.allclose(filtered_cpu, filtered_gpu)
E   assert False
E    +  where False = <function allclose at 0x0000024AC9416F30>(tensor([[[[-0.1065,  0.0101,  0.3248,  1.3305,  0.8408,  0.8408,  0.3901,\n            0.1044, -0.1105, -0.1105,  0.2652,  0.2652,  0.3826,  0.2593,\n            0.6134]]]]), tensor([[[[-0.1065, -0.1065,  1.3305,  0.3248,  1.9730,  0.8408,  0.1044,\n            0.3901, -0.3847, -0.1105,  0.2652,  0.3826, -2.2472,  0.6134,\n            0.2593]]]]))
E    +    where <function allclose at 0x0000024AC9416F30> = np.allclose
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\triton\language\core.py:43: in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\core.py:1638: in arange
    return _semantic.arange(start, end)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\language\semantic.py:583: in arange
    raise ValueError("arange's range must be a power of 2")
E   ValueError: arange's range must be a power of 2

The above exception was the direct cause of the following exception:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:115: in median_filter_cuda
    kernel[(grid_size,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)
venv\Lib\site-packages\triton\runtime\jit.py:419: in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:733: in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\runtime\jit.py:861: in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:300: in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\triton\compiler\compiler.py:80: in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
E   triton.compiler.errors.CompilationError: at 5:14:
E   def kernel(
E       y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr
E   ):  # x.shape[-1] == filter_width
E       row_idx = tl.program_id(0)
E       offsets = tl.arange(0, BLOCK_SIZE)
E                 ^
E   arange's range must be a power of 2
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.12s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1338 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

7 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.numel() // slices.shape[-1]

    kernel = median_kernel(filter_width)
    y = torch.empty_like(slices[..., 0])

    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ..............FF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                 ^^^^^^^^
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:13: in test_transcribe
    model = whisper.load_model(model_name).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\__init__.py:127: in load_model
    checkpoint = torch.load(fp, map_location=device, **load_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1534: in load
    return _load(
venv\Lib\site-packages\torch\serialization.py:2135: in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_weights_only_unpickler.py:536: in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:2099: in persistent_load
    typed_storage = load_tensor(
venv\Lib\site-packages\torch\serialization.py:2065: in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:1872: in restore_location
    return default_restore_location(storage, map_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:704: in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\serialization.py:638: in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\storage.py:291: in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\_utils.py:103: in _to
    untyped_storage.copy_(self, non_blocking)
E   torch.AcceleratorError: CUDA error: an illegal memory access was encountered
E   Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 12.08s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1221 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

8 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.unfold(-1, filter_width, 1)
    grid = slices.shape[0] * slices.shape[1]
    y = torch.empty_like(slices[..., 0])
    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()
    kernel[(grid,)](y, slices, filter_width, BLOCK_SIZE=BLOCK_SIZE)
    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, filter_width, BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, filter_width, BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, filter_width, BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:111: in median_filter_cuda
    kernel[(grid,)](y, slices, filter_width, BLOCK_SIZE=BLOCK_SIZE)
    ^^^^^^
E   NameError: name 'kernel' is not defined
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.39s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1335 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

9 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
import torch
import numpy as np
from numba import cuda

@cuda.jit
def median_kernel(y, x, stride_x, stride_y, block_size, filter_width):
    idx = cuda.grid(1)
    block_start = idx * block_size
    block_end = min(block_start + block_size, y.size)
    for i in range(block_start, block_end):
        start = i * stride_y
        end = start + filter_width
        y[i] = np.median(x[start:end:stride_x])


def median_filter_cuda(x: torch.Tensor, filter_width: int):
    slices = x.contiguous().unfold(-1, filter_width, 1)
    grid = (slices.numel() + (filter_width - 1)) // filter_width

    y = torch.empty(slices.shape[:-1], device=x.device, dtype=x.dtype)

    block_size = min(grid, 1024)  # Limit block size to CUDA max
    threads_per_block = 256  # Use 256 threads per block for better balance

    median_kernel[(grid,), (threads_per_block,)](
        y.view(-1), x.view(-1), x.stride(-2), y.stride(-1), filter_width
    )

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    median_kernel[(grid,), (threads_per_block,)](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape1] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    median_kernel[(grid,), (threads_per_block,)](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape2] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    median_kernel[(grid,), (threads_per_block,)](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
___________________ test_median_filter_equivalence[shape3] ____________________
venv\Lib\site-packages\numba\cuda\dispatcher.py:695: in typeof_pyval
    return typeof(val, Purpose.argument)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\core\typing\typeof.py:37: in typeof
    raise ValueError(msg)
E   ValueError: [1mCannot determine Numba type of <class 'torch.Tensor'>[0m

During handling of the above exception, another exception occurred:
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:34: in median_filter
    return median_filter_cuda(x, filter_width)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:130: in median_filter_cuda
    median_kernel[(grid,), (threads_per_block,)](
venv\Lib\site-packages\numba\cuda\dispatcher.py:539: in __call__
    return self.dispatcher.call(args, self.griddim, self.blockdim,
venv\Lib\site-packages\numba\cuda\dispatcher.py:681: in call
    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:688: in _compile_for_args
    argtypes = [self.typeof_pyval(a) for a in args]
                ^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\dispatcher.py:700: in typeof_pyval
    return typeof(cuda.as_cuda_array(val, sync=False),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\api.py:76: in as_cuda_array
    return from_cuda_array_interface(obj.__cuda_array_interface__,
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:231: in _require_cuda_context
    with _runtime.ensure_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\devices.py:121: in ensure_context
    with driver.get_active_context():
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:505: in __enter__
    driver.cuCtxGetDevice(byref(hdevice))
venv\Lib\site-packages\numba\cuda\cudadrv\driver.py:326: in safe_cuda_api_call
    retcode = libfn(*args)
              ^^^^^^^^^^^^
E   OSError: exception: access violation reading 0xFFFFFFFFFFFFFFFF
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
============================== warnings summary ===============================
tests/test_timing.py::test_median_filter_equivalence[shape0]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 10 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

tests/test_timing.py::test_median_filter_equivalence[shape1]
  c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\venv\Lib\site-packages\numba\cuda\dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 15 will likely result in GPU under-utilization due to low occupancy.[0m
    warn(NumbaPerformanceWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
================== 6 failed, 20 passed, 2 warnings in 13.37s ==================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1397 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

10 failed optimizations : regenerating attempt...
OptimizationError
Error optimizing whisper with 4o: Failed to optimize code object
def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """Apply a median filter of given width along the last dimension of x"""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y

with optimizer '4o' after 10 attempts
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\tokenizer.py, startline : 329
from functools import lru_cache
import os
import base64
import tiktoken

@lru_cache(maxsize=None)
def get_encoding(name: str = "gpt2", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), "assets", f"{name}.tiktoken")

    with open(vocab_path, 'r') as file:
        ranks = {base64.b64decode(token): int(rank) for token, rank in (line.split() for line in file if line)}

    n_vocab = len(ranks)
    special_tokens = {}
    LANGUAGES = LANGUAGES  # Assuming LANGUAGES is defined somewhere globally, if not this needs to be sourced properly

    specials = [
        "<|endoftext|>",
        "<|startoftranscript|>",
        *[f"<|{lang}|>" for lang in list(LANGUAGES.keys())[:num_languages]],
        "<|translate|>",
        "<|transcribe|>",
        "<|startoflm|>",
        "<|startofprev|>",
        "<|nospeech|>",
        "<|notimestamps|>",
        *[f"<|{i * 0.02:.2f}|>" for i in range(1501)],
    ]

    special_tokens.update({token: n_vocab + i for i, token in enumerate(specials)})

    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab + len(specials),
        pat_str=r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ................                   [ 80%]
projects\whisper\tests\test_tokenizer.py FFFF                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
____________________________ test_tokenizer[True] _____________________________
projects\whisper\tests\test_tokenizer.py:8: in test_tokenizer
    tokenizer = get_tokenizer(multilingual=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:377: in get_tokenizer
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:344: in get_encoding
    LANGUAGES = LANGUAGES  # Assuming LANGUAGES is defined somewhere globally, if not this needs to be sourced properly
                ^^^^^^^^^
E   UnboundLocalError: cannot access local variable 'LANGUAGES' where it is not associated with a value
____________________________ test_tokenizer[False] ____________________________
projects\whisper\tests\test_tokenizer.py:8: in test_tokenizer
    tokenizer = get_tokenizer(multilingual=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:377: in get_tokenizer
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:344: in get_encoding
    LANGUAGES = LANGUAGES  # Assuming LANGUAGES is defined somewhere globally, if not this needs to be sourced properly
                ^^^^^^^^^
E   UnboundLocalError: cannot access local variable 'LANGUAGES' where it is not associated with a value
_________________________ test_multilingual_tokenizer _________________________
projects\whisper\tests\test_tokenizer.py:15: in test_multilingual_tokenizer
    gpt2_tokenizer = get_tokenizer(multilingual=False)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:377: in get_tokenizer
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:344: in get_encoding
    LANGUAGES = LANGUAGES  # Assuming LANGUAGES is defined somewhere globally, if not this needs to be sourced properly
                ^^^^^^^^^
E   UnboundLocalError: cannot access local variable 'LANGUAGES' where it is not associated with a value
____________________________ test_split_on_unicode ____________________________
projects\whisper\tests\test_tokenizer.py:28: in test_split_on_unicode
    multilingual_tokenizer = get_tokenizer(multilingual=True)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:377: in get_tokenizer
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\tokenizer.py:344: in get_encoding
    LANGUAGES = LANGUAGES  # Assuming LANGUAGES is defined somewhere globally, if not this needs to be sourced properly
                ^^^^^^^^^
E   UnboundLocalError: cannot access local variable 'LANGUAGES' where it is not associated with a value
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_tokenizer.py::test_tokenizer[True] - Unbou...
FAILED projects\whisper\tests\test_tokenizer.py::test_tokenizer[False] - Unbo...
FAILED projects\whisper\tests\test_tokenizer.py::test_multilingual_tokenizer
FAILED projects\whisper\tests\test_tokenizer.py::test_split_on_unicode - Unbo...
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.90s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile8.speedscope'. Samples: 1517 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 14.061
Running py-spy profiler...
Benchmark 2 complete with duration 14.105
Running py-spy profiler...
Benchmark 3 complete with duration 13.957
Running py-spy profiler...
Benchmark 4 complete with duration 13.957
Running py-spy profiler...
Benchmark 5 complete with duration 14.053
Running py-spy profiler...
Benchmark 6 complete with duration 13.797
Running py-spy profiler...
Benchmark 7 complete with duration 13.866
Running py-spy profiler...
Benchmark 8 complete with duration 13.983
Running py-spy profiler...
Benchmark 9 complete with duration 14.09
Running py-spy profiler...
Benchmark 10 complete with duration 13.841

Done with BASE prompting - moving to next prompt type for project whisper with optimizer 4o

Done with 4o - moving to next optimizer...
GENERATED META PROMPT: 
Optimize the following code object to achieve the best runtime performance while preserving its existing signature and interface. Consider the following aspects in your optimization: algorithmic complexity (big O), efficiency of data structures, loop optimization, memory access patterns, I/O operations, parallel processing, and removal of any redundant computations. After thorough analysis and reasoning, return only the final optimized version of the code object.

```python
def transcribe_audio(input_data):
    # Simulated function to transcribe audio data into text
    transcription = []
    for i in range(len(input_data)):
        # Extract features from the audio data
        features = extract_features(input_data[i])
        # Convert features to transcription
        text = convert_to_text(features)
        transcription.append(text)
    return transcription
```

Ensure that the optimized solution fits within the context of the 'whisper' project, and provides enhanced performance using 'gemini-v25-pro's capabilities to apply complex reasoning and performance optimizations.
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\triton_ops.py, startline : 105
def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """Apply a median filter of given width along the last dimension of x"""
    if x.shape[-1] < filter_width:
        output_shape = x.shape[:-1] + (0,)
        return torch.empty(output_shape, device=x.device, dtype=x.dtype)

    x_cont = x.contiguous()

    output_last_dim = x_cont.shape[-1] - filter_width + 1
    output_shape = x_cont.shape[:-1] + (output_last_dim,)
    y = torch.empty(output_shape, device=x.device, dtype=x.dtype)

    grid_dim = math.prod(x_cont.shape[:-1]) if x.ndim > 1 else 1
    if grid_dim == 0:
        return y

    if y.ndim > 1:
        threads_per_row = y.stride(-2)
        if threads_per_row > 0:
            block_size = 1 << (threads_per_row - 1).bit_length()
            block_size = min(block_size, 1024)
        else:
            block_size = 32
    else:
        block_size = 256
        
    kernel = median_kernel(filter_width)

    stride_x = x_cont.stride(-2) if x.ndim > 1 else 0
    stride_y = y.stride(-2) if y.ndim > 1 else 0
    
    kernel[(grid_dim,)](y, x_cont, stride_x, stride_y, block_size=block_size)

    return y
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ............FFFF                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
___________________ test_median_filter_equivalence[shape0] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:42: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:118: in median_filter_cuda
    grid_dim = math.prod(x_cont.shape[:-1]) if x.ndim > 1 else 1
               ^^^^
E   NameError: name 'math' is not defined. Did you forget to import 'math'
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:42: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:118: in median_filter_cuda
    grid_dim = math.prod(x_cont.shape[:-1]) if x.ndim > 1 else 1
               ^^^^
E   NameError: name 'math' is not defined. Did you forget to import 'math'
___________________ test_median_filter_equivalence[shape2] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:42: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:118: in median_filter_cuda
    grid_dim = math.prod(x_cont.shape[:-1]) if x.ndim > 1 else 1
               ^^^^
E   NameError: name 'math' is not defined. Did you forget to import 'math'
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:94: in test_median_filter_equivalence
    filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:42: in median_filter
    result = median_filter_cuda(x, filter_width)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\triton_ops.py:118: in median_filter_cuda
    grid_dim = math.prod(x_cont.shape[:-1]) if x.ndim > 1 else 1
               ^^^^
E   NameError: name 'math' is not defined. Did you forget to import 'math'
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape0]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape2]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 12.46s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile6.speedscope'. Samples: 1284 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 14.078
Running py-spy profiler...
Benchmark 2 complete with duration 14.145
Running py-spy profiler...
Benchmark 3 complete with duration 14.406
Running py-spy profiler...
Benchmark 4 complete with duration 14.217
Running py-spy profiler...
Benchmark 5 complete with duration 14.09
Running py-spy profiler...
Benchmark 6 complete with duration 14.187
Running py-spy profiler...
Benchmark 7 complete with duration 14.032
Running py-spy profiler...
Benchmark 8 complete with duration 14.252
Running py-spy profiler...
Benchmark 9 complete with duration 14.403
Running py-spy profiler...
Benchmark 10 complete with duration 14.27

Done with MP prompting - moving to next prompt type for project whisper with optimizer 25

Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 18
def median_filter(x: torch.Tensor, filter_width: int):
    """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), "`filter_width` should be an odd number"

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    # Store original shape and ensure tensor is at least 3D for F.pad's 'reflect' mode
    original_shape = x.shape
    if x.ndim < 3:
        x = x.view((1,) * (3 - x.ndim) + original_shape)

    # Pad only the last dimension
    x = F.pad(x, (pad_width, pad_width), mode="reflect")

    result = None
    if x.is_cuda:
        try:
            from .triton_ops import median_filter_cuda

            result = median_filter_cuda(x, filter_width)
        except (ImportError, RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                "Failed to launch Triton kernels, likely due to missing CUDA toolkit; "
                "falling back to a slower median kernel implementation..."
            )

    if result is None:
        # .unfold() creates a view of sliding windows, which is memory-efficient.
        # .sort().values is generally faster than torch.median() for this task.
        windows = x.unfold(-1, filter_width, 1)
        result = windows.sort(dim=-1).values[..., pad_width]

    # Reshape back to the original number of dimensions
    if len(original_shape) < 3:
        final_shape = original_shape[:-1] + (-1,)
        result = result.view(final_shape)

    return result
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ...........F...F                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
_________________________ test_median_filter[shape3] __________________________
projects\whisper\tests\test_timing.py:72: in test_median_filter
    filtered = median_filter(x, filter_width)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:93: in test_median_filter_equivalence
    filtered_cpu = median_filter(x, filter_width)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter[shape3] - No...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 4 failed, 22 passed in 7.52s =========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile1.speedscope'. Samples: 835 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    # More direct and efficient creation of the skewed input matrix.
    x_skew = F.pad(x, (0, M), value=float('inf'))
    x_skew = x_skew.T.contiguous()

    # Create tensors directly on the target device to avoid host->device copy.
    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    # Simplified and more readable extraction of the relevant part of the trace matrix.
    trace = trace.T[:M + 1, :N + 1]
    
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,29) (2,27)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,44) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1584) (2,1576)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,397) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.17s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1463 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
    )
    x_skew = x_skew.T.contiguous()

    # Efficiently initialize cost matrix directly on the target device
    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    # Optimized trace extraction with a direct slice, avoiding inefficient operations
    trace = trace.T[: M + 1, : N + 1]
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,28) (2,27)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,45) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1588) (2,1587)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,397) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.36s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1529 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

2 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    device = x.device

    x_skew = (
        F.pad(x, (0, M + 1), value=torch.inf).flatten()[: M * (N + M)].reshape(M, N + M)
    )
    x_skew = x_skew.T.contiguous()

    cost = torch.full((N + M + 2, M + 2), torch.inf, device=device)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), dtype=torch.int32, device=device)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    final_trace = trace.T[:M + 1, :N + 1]
    return backtrace(final_trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,27) (2,25)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,43) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1582) (2,1589)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,398) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.11s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1501 Errors: 1
py-spy> Visit https://www.speedscope.app/ to view

3 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = F.pad(x, (0, M), value=torch.inf).T.contiguous()

    cost = torch.full((N + M + 2, M + 2), torch.inf, dtype=x.dtype, device=x.device)
    cost[0, 0] = 0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    trace = trace[:N + 1, :M + 1].T
    return backtrace(trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,27) (2,28)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,42) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1578) (2,1586)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,401) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.31s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1545 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

4 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    import torch
    import torch.nn.functional as F

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than BLOCK_SIZE={BLOCK_SIZE}"

    device = x.device
    dtype = x.dtype

    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
        .T.contiguous()
    )

    cost = torch.full((N + M + 2, M + 2), float('inf'), device=device, dtype=dtype)
    cost[0, 0] = 0.0
    trace = torch.zeros((N + M + 2, M + 2), device=device, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    final_trace = trace.T[:M + 1, :N + 1]
    return backtrace(final_trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
E   assert False
E    +  where False = <function allclose at 0x000002ACFF856F30>(array([[ 0,  0,  0,  1,  1,  1,  1,  1,  1,  2,  3,  3,  3,  4,  5,  6,\n         6,  6,  7,  7,  7,  7,  7,  7,  8,  9...     [ 0,  1,  2,  2,  3,  4,  5,  6,  7,  7,  7,  8,  9, 10, 10, 10,\n        11, 12, 13, 14, 15, 16, 17, 18, 19, 19]]), array([[-1,  0,  0,  1,  1,  2,  2,  3,  4,  4,  5,  5,  6,  6,  6,  7,\n         7,  7,  7,  7,  8,  8,  9,  9,  9,  9...     [ 0,  1,  2,  2,  3,  3,  4,  5,  5,  6,  6,  7,  8,  9, 10, 10,\n        11, 12, 13, 14, 15, 16, 16, 17, 18, 19]]))
E    +    where <function allclose at 0x000002ACFF856F30> = np.allclose
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,43) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1584) (2,1578)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,399) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 14.35s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1503 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

5 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 116
def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
        .T.contiguous()
    )

    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0.0
    trace = torch.zeros_like(cost, dtype=torch.int32)

    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    final_trace = trace.T[:M + 1, :N + 1]
    return backtrace(final_trace.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,24) (2,25)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,44) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,1585) (2,1583)
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,399) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 15.12s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1480 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

6 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 14.165
Running py-spy profiler...
Benchmark 2 complete with duration 14.289
Running py-spy profiler...
Benchmark 3 complete with duration 14.221
Running py-spy profiler...
Benchmark 4 complete with duration 14.089
Running py-spy profiler...
Benchmark 5 complete with duration 13.956
Running py-spy profiler...
Benchmark 6 complete with duration 14.102
Running py-spy profiler...
Benchmark 7 complete with duration 14.153
Running py-spy profiler...
Benchmark 8 complete with duration 14.485
Running py-spy profiler...
Benchmark 9 complete with duration 14.304
Running py-spy profiler...
Benchmark 10 complete with duration 14.266

Done with FS prompting - moving to next prompt type for project whisper with optimizer 25

Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 118
import torch
import torch.nn.functional as F

def dtw_cuda(x, BLOCK_SIZE=1024):
    """Optimized CUDA implementation of Dynamic Time Warping.

    This version optimizes the original by:
    1. Creating tensors directly on the target GPU device to avoid redundant CPU->GPU data transfers.
    2. Replacing a complex and inefficient slicing/reshaping of the output `trace` tensor
       with a simple, direct slice and transpose operation, reducing memory overhead and
       improving clarity.
    """
    from .triton_ops import dtw_kernel

    M, N = x.shape
    assert M < BLOCK_SIZE, f"M should be smaller than {BLOCK_SIZE=}"

    # The skew operation is a key part of the parallel algorithm.
    # The original chain of PyTorch ops is kept as it is likely efficient.
    x_skew = (
        F.pad(x, (0, M + 1), value=float('inf'))
        .flatten()[: M * (N + M)]
        .reshape(M, N + M)
    )
    x_skew = x_skew.T.contiguous()

    # OPTIMIZATION: Create tensors directly on the target device.
    cost = torch.full((N + M + 2, M + 2), float('inf'), device=x.device, dtype=x.dtype)
    cost[0, 0] = 0
    trace = torch.zeros((N + M + 2, M + 2), device=x.device, dtype=torch.int32)

    # Kernel launch remains the same.
    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0),
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    # OPTIMIZATION: Replaced complex flatten/reshape with a direct slice and transpose.
    # This extracts the core (N+1, M+1) matrix and transposes it to (M+1, N+1).
    trace_result = trace[1 : N + 2, 1 : M + 2].T

    # Data transfer to CPU is unavoidable if backtrace is a CPU-based function.
    return backtrace(trace_result.cpu().numpy())
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py ....FFFF........                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
______________________ test_dtw_cuda_equivalence[10-20] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,28) (2,25)
______________________ test_dtw_cuda_equivalence[32-16] _______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,42) (2,32)
_____________________ test_dtw_cuda_equivalence[123-1500] _____________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
E   assert False
E    +  where False = <function allclose at 0x0000017C47C07330>(array([[   0,    0,    0, ...,  120,  121,  122],\n       [   0,    1,    2, ..., 1499, 1499, 1499]], shape=(2, 1579)), array([[  -1,   -1,   -1, ...,  120,  121,  122],\n       [   0,    1,    2, ..., 1498, 1498, 1499]], shape=(2, 1579)))
E    +    where <function allclose at 0x0000017C47C07330> = np.allclose
_____________________ test_dtw_cuda_equivalence[234-189] ______________________
projects\whisper\tests\test_timing.py:64: in test_dtw_cuda_equivalence
    assert np.allclose(trace_cpu, trace_cuda)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\numpy\_core\numeric.py:2496: in isclose
    result = (less_equal(abs(x - y), atol + rtol * abs(y))
                             ^^^^^
E   ValueError: operands could not be broadcast together with shapes (2,390) (2,234)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[10-20]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[32-16]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[123-1500]
FAILED projects\whisper\tests\test_timing.py::test_dtw_cuda_equivalence[234-189]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 13.06s ========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile2.speedscope'. Samples: 1339 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 13.199
Running py-spy profiler...
Benchmark 2 complete with duration 13.023
Running py-spy profiler...
Benchmark 3 complete with duration 12.998
Running py-spy profiler...
Benchmark 4 complete with duration 13.104
Running py-spy profiler...
Benchmark 5 complete with duration 12.935
Running py-spy profiler...
Benchmark 6 complete with duration 13.118
Running py-spy profiler...
Benchmark 7 complete with duration 13.159
Running py-spy profiler...
Benchmark 8 complete with duration 13.121
Running py-spy profiler...
Benchmark 9 complete with duration 13.191
Running py-spy profiler...
Benchmark 10 complete with duration 13.02

Done with COT prompting - moving to next prompt type for project whisper with optimizer 25

Optimizing...
Running py-spy profiler...
============FAULTY CODE============
filename : whisper\timing.py, startline : 18
def median_filter(x: torch.Tensor, filter_width: int):
    """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), "`filter_width` should be an odd number"

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    ndim = x.ndim
    if ndim <= 2:
        # `F.pad` with "reflect" mode supports 3D, 4D, and 5D inputs
        x = x[None, None, :]

    # Pad only the last dimension
    x = F.pad(x, (pad_width, pad_width), mode="reflect")

    result = None
    if x.is_cuda:
        try:
            from .triton_ops import median_filter_cuda

            result = median_filter_cuda(x, filter_width)
        except (RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                "Failed to launch Triton kernels, likely due to missing CUDA toolkit; "
                "falling back to a slower median kernel implementation..."
            )

    if result is None:
        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)
        result = x.unfold(-1, filter_width, 1).sort()[0][..., pad_width]

    if ndim <= 2:
        result = result[0, 0]

    return result
===================================
py-spy> Sampling process 100 times a second. Press Control-C to exit.

============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.1, pluggy-1.6.0
rootdir: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\projects\whisper
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 26 items

projects\whisper\tests\test_audio.py F                                   [  3%]
projects\whisper\tests\test_normalizer.py ....                           [ 19%]
projects\whisper\tests\test_timing.py .........F.F.F.F                   [ 80%]
projects\whisper\tests\test_tokenizer.py ....                            [ 96%]
projects\whisper\tests\test_transcribe.py F                              [100%]

================================== FAILURES ===================================
_________________________________ test_audio __________________________________
projects\whisper\tests\test_audio.py:10: in test_audio
    audio = load_audio(audio_path)
            ^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
_________________________ test_median_filter[shape1] __________________________
projects\whisper\tests\test_timing.py:72: in test_median_filter
    filtered = median_filter(x, filter_width)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
_________________________ test_median_filter[shape3] __________________________
projects\whisper\tests\test_timing.py:72: in test_median_filter
    filtered = median_filter(x, filter_width)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
___________________ test_median_filter_equivalence[shape1] ____________________
projects\whisper\tests\test_timing.py:93: in test_median_filter_equivalence
    filtered_cpu = median_filter(x, filter_width)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
___________________ test_median_filter_equivalence[shape3] ____________________
projects\whisper\tests\test_timing.py:93: in test_median_filter_equivalence
    filtered_cpu = median_filter(x, filter_width)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\timing.py:37: in median_filter
    x = F.pad(x, (pad_width, pad_width), mode="reflect")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv\Lib\site-packages\torch\nn\functional.py:5406: in pad
    return torch._C._nn.pad(input, pad, mode, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   NotImplementedError: Padding size 2 is not supported for 4D input tensor.
E   Supported combinations for non-constant padding:
E     - 2D or 3D input: padding size = 2 (pads last dimension)
E     - 3D or 4D input: padding size = 4 (pads last 2 dimensions)
E     - 4D or 5D input: padding size = 6 (pads last 3 dimensions)
__________________________ test_transcribe[tiny.en] ___________________________
projects\whisper\tests\test_transcribe.py:17: in test_transcribe
    result = model.transcribe(
projects\whisper\whisper\transcribe.py:139: in transcribe
    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:140: in log_mel_spectrogram
    audio = load_audio(audio)
            ^^^^^^^^^^^^^^^^^
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
E   FileNotFoundError: [WinError 2] The system cannot find the file specified
- generated xml file: c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\temp\report.xml -
=========================== short test summary info ===========================
FAILED projects\whisper\tests\test_audio.py::test_audio - FileNotFoundError: ...
FAILED projects\whisper\tests\test_timing.py::test_median_filter[shape1] - No...
FAILED projects\whisper\tests\test_timing.py::test_median_filter[shape3] - No...
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape1]
FAILED projects\whisper\tests\test_timing.py::test_median_filter_equivalence[shape3]
FAILED projects\whisper\tests\test_transcribe.py::test_transcribe[tiny.en] - ...
======================== 6 failed, 20 passed in 7.56s =========================

py-spy> Stopped sampling because process exited
py-spy> Wrote speedscope file to 'c:\Users\aiden\OneDrive\Desktop\personalprojects\ahmedlab\MPCO-dupe\pipeline\profiler\profiles\whisper_profile1.speedscope'. Samples: 857 Errors: 0
py-spy> Visit https://www.speedscope.app/ to view

1 failed optimizations : regenerating attempt...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizing...
Running py-spy profiler...
Optimizations generated - benchmarking...
Running py-spy profiler...
Benchmark 1 complete with duration 13.769
Running py-spy profiler...
Benchmark 2 complete with duration 14.319
Running py-spy profiler...
Benchmark 3 complete with duration 13.236
Running py-spy profiler...
Benchmark 4 complete with duration 13.795
Running py-spy profiler...
Benchmark 5 complete with duration 12.935
Running py-spy profiler...
Benchmark 6 complete with duration 13.064
Running py-spy profiler...
Benchmark 7 complete with duration 12.986
Running py-spy profiler...
Benchmark 8 complete with duration 13.666
Running py-spy profiler...
Benchmark 9 complete with duration 13.235
Running py-spy profiler...
Benchmark 10 complete with duration 13.034

Done with BASE prompting - moving to next prompt type for project whisper with optimizer 25

Done with 25 - moving to next optimizer...
