0
"def median_filter(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""

    pad_width = filter_width // 2

    if x.shape[-1] <= pad_width:

        # F.pad requires the padding width to be smaller than the input dimension

        return x



    if (ndim := x.ndim) <= 2:

        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D

        x = x[None, None, :]



    assert (

        filter_width > 0 and filter_width % 2 == 1

    ), ""`filter_width` should be an odd number""



    result = None

    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=""reflect"")

    if x.is_cuda:

        try:

            from .triton_ops import median_filter_cuda



            result = median_filter_cuda(x, filter_width)

        except (RuntimeError, subprocess.CalledProcessError):

            warnings.warn(

                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""

                ""falling back to a slower median kernel implementation...""

            )



    if result is None:

        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)

        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]



    if ndim <= 2:

        result = result[0, 0]



    return result
"
"def median_filter(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of width `filter_width` along the last dimension of `x`""""""
    
    # Early validation to avoid computation overhead
    assert filter_width > 0 and filter_width % 2 == 1, ""`filter_width` should be an odd number""
    
    pad_width = filter_width // 2
    
    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    # Cache ndim to avoid repeated attribute access
    ndim = x.ndim
    original_shape = None
    
    if ndim <= 2:
        # F.pad does not support 1D or 2D inputs for reflect padding but supports 3D and 4D
        original_shape = x.shape  # Store original shape for efficient restoration
        x = x.view(1, 1, -1) if ndim == 1 else x.view(1, 1, x.shape[0], x.shape[1])

    # Apply padding once with optimized parameters
    x_padded = F.pad(x, (pad_width, pad_width, 0, 0), mode=""reflect"")
    
    result = None
    
    # Prioritize CUDA kernel if available for maximum performance
    if x.is_cuda:
        try:
            from .triton_ops import median_filter_cuda
            result = median_filter_cuda(x_padded, filter_width)
        except (RuntimeError, subprocess.CalledProcessError):
            warnings.warn(
                ""Failed to launch Triton kernels, likely due to missing CUDA toolkit; ""
                ""falling back to a slower median kernel implementation...""
            )

    if result is None:
        # Optimized fallback: unfold + sort is faster than torch.median
        # Use contiguous() to optimize memory access patterns for better cache performance
        unfolded = x_padded.unfold(-1, filter_width, 1).contiguous()
        # Direct indexing after sort is more efficient than multiple tensor operations
        result = unfolded.sort(dim=-1)[0][..., pad_width]

    # Efficient shape restoration for low-dimensional inputs
    if ndim <= 2:
        if ndim == 1:
            result = result.view(-1)
        else:
            result = result.view(original_shape)

    return result"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def dtw_cuda(x, BLOCK_SIZE=1024):

    from .triton_ops import dtw_kernel



    M, N = x.shape

    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""



    x_skew = (

        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)

    )

    x_skew = x_skew.T.contiguous()

    cost = torch.ones(N + M + 2, M + 2) * np.inf

    cost[0, 0] = 0

    cost = cost.to(x.device)

    trace = torch.zeros_like(cost, dtype=torch.int32)



    dtw_kernel[(1,)](

        cost,

        trace,

        x_skew,

        x_skew.stride(0),

        cost.stride(0),

        trace.stride(0),

        N,

        M,

        BLOCK_SIZE=BLOCK_SIZE,

    )



    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[

        :, : N + 1

    ]

    return backtrace(trace.cpu().numpy())
"
"def dtw_cuda(x, BLOCK_SIZE=1024):
    from .triton_ops import dtw_kernel
    
    M, N = x.shape
    assert M < BLOCK_SIZE, f""M should be smaller than {BLOCK_SIZE=}""
    
    # Pre-allocate with exact size to avoid unnecessary memory allocation
    padded_size = M * (N + M)
    
    # Optimize skewing operation by avoiding intermediate tensor creation
    x_flat = F.pad(x, (0, M + 1), value=np.inf).flatten()
    x_skew = x_flat[:padded_size].view(M, N + M).t().contiguous()
    
    # Pre-allocate tensors on target device to avoid data transfers
    device = x.device
    cost_shape = (N + M + 2, M + 2)
    
    # Use fill_ for better memory efficiency than multiplication
    cost = torch.empty(cost_shape, device=device, dtype=x.dtype)
    cost.fill_(np.inf)
    cost[0, 0] = 0
    
    # Create trace tensor directly on device with correct dtype
    trace = torch.zeros(cost_shape, device=device, dtype=torch.int32)
    
    # Kernel launch with optimized parameters
    dtw_kernel[(1,)](
        cost,
        trace,
        x_skew,
        x_skew.stride(0),
        cost.stride(0), 
        trace.stride(0),
        N,
        M,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    
    # Optimize trace reshaping with pre-calculated dimensions
    trace_flat = trace.t().flatten()
    target_size = (M + 1) * (M + N + 3)
    reshaped_trace = trace_flat[:target_size].view(M + 1, M + N + 3)[:, :N + 1]
    
    return backtrace(reshaped_trace.cpu().numpy())"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def load_model(

    name: str,

    device: Optional[Union[str, torch.device]] = None,

    download_root: str = None,

    in_memory: bool = False,

) -> Whisper:

    """"""

    Load a Whisper ASR model



    Parameters

    ----------

    name : str

        one of the official model names listed by `whisper.available_models()`, or

        path to a model checkpoint containing the model dimensions and the model state_dict.

    device : Union[str, torch.device]

        the PyTorch device to put the model into

    download_root: str

        path to download the model files; by default, it uses ""~/.cache/whisper""

    in_memory: bool

        whether to preload the model weights into host memory



    Returns

    -------

    model : Whisper

        The Whisper ASR model instance

    """"""



    if device is None:

        device = ""cuda"" if torch.cuda.is_available() else ""cpu""

    if download_root is None:

        default = os.path.join(os.path.expanduser(""~""), "".cache"")

        download_root = os.path.join(os.getenv(""XDG_CACHE_HOME"", default), ""whisper"")



    if name in _MODELS:

        checkpoint_file = _download(_MODELS[name], download_root, in_memory)

        alignment_heads = _ALIGNMENT_HEADS[name]

    elif os.path.isfile(name):

        checkpoint_file = open(name, ""rb"").read() if in_memory else name

        alignment_heads = None

    else:

        raise RuntimeError(

            f""Model {name} not found; available models = {available_models()}""

        )



    with (

        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, ""rb"")

    ) as fp:

        kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}

        checkpoint = torch.load(fp, map_location=device, **kwargs)

    del checkpoint_file



    dims = ModelDimensions(**checkpoint[""dims""])

    model = Whisper(dims)

    model.load_state_dict(checkpoint[""model_state_dict""])



    if alignment_heads is not None:

        model.set_alignment_heads(alignment_heads)



    return model.to(device)
"
"def load_model(
    name: str,
    device: Optional[Union[str, torch.device]] = None,
    download_root: str = None,
    in_memory: bool = False,
) -> Whisper:
    """"""
    Load a Whisper ASR model

    Parameters
    ----------
    name : str
        one of the official model names listed by `whisper.available_models()`, or
        path to a model checkpoint containing the model dimensions and the model state_dict.
    device : Union[str, torch.device]
        the PyTorch device to put the model into
    download_root: str
        path to download the model files; by default, it uses ""~/.cache/whisper""
    in_memory: bool
        whether to preload the model weights into host memory

    Returns
    -------
    model : Whisper
        The Whisper ASR model instance
    """"""
    
    # Optimize device selection with early CUDA availability check
    if device is None:
        device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    
    # Cache-friendly download root computation - avoid redundant path operations
    if download_root is None:
        xdg_cache = os.getenv(""XDG_CACHE_HOME"")
        if xdg_cache:
            download_root = os.path.join(xdg_cache, ""whisper"")
        else:
            download_root = os.path.join(os.path.expanduser(""~""), "".cache"", ""whisper"")
    
    # Early path validation to avoid unnecessary operations
    is_model_name = name in _MODELS
    is_file_path = not is_model_name and os.path.isfile(name)
    
    if not is_model_name and not is_file_path:
        raise RuntimeError(
            f""Model {name} not found; available models = {available_models()}""
        )
    
    # Optimized checkpoint loading with single-pass file handling
    if is_model_name:
        checkpoint_file = _download(_MODELS[name], download_root, in_memory)
        alignment_heads = _ALIGNMENT_HEADS[name]
    else:  # is_file_path
        if in_memory:
            # Use context manager for automatic resource cleanup
            with open(name, ""rb"") as f:
                checkpoint_file = f.read()
        else:
            checkpoint_file = name
        alignment_heads = None
    
    # Streamlined checkpoint loading with optimized torch.load parameters
    try:
        if in_memory:
            # BytesIO for in-memory data, single context manager
            with io.BytesIO(checkpoint_file) as fp:
                # Pre-compute kwargs to avoid version check overhead during loading
                kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}
                checkpoint = torch.load(fp, map_location=device, **kwargs)
        else:
            # Direct file access for disk-based loading
            kwargs = {""weights_only"": True} if torch.__version__ >= ""1.13"" else {}
            checkpoint = torch.load(checkpoint_file, map_location=device, **kwargs)
    finally:
        # Ensure memory cleanup for in-memory checkpoints
        if in_memory and 'checkpoint_file' in locals():
            del checkpoint_file
    
    # Direct model construction and state loading
    dims = ModelDimensions(**checkpoint[""dims""])
    model = Whisper(dims)
    model.load_state_dict(checkpoint[""model_state_dict""])
    
    # Conditional alignment heads setting
    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)
    
    # Direct device transfer - model.to() is idempotent if already on target device
    return model.to(device)"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def __init__(self, dims: ModelDimensions):

    super().__init__()

    self.dims = dims

    self.encoder = AudioEncoder(

        self.dims.n_mels,

        self.dims.n_audio_ctx,

        self.dims.n_audio_state,

        self.dims.n_audio_head,

        self.dims.n_audio_layer,

    )

    self.decoder = TextDecoder(

        self.dims.n_vocab,

        self.dims.n_text_ctx,

        self.dims.n_text_state,

        self.dims.n_text_head,

        self.dims.n_text_layer,

    )

    # use the last half among the decoder layers for time alignment by default;

    # to use a specific set of heads, see `set_alignment_heads()` below.

    all_heads = torch.zeros(

        self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool

    )

    all_heads[self.dims.n_text_layer // 2 :] = True

    self.register_buffer(""alignment_heads"", all_heads.to_sparse(), persistent=False)
"
"def __init__(self, dims: ModelDimensions):
    super().__init__()
    
    self.dims = dims
    
    # Cache dimension values to avoid repeated attribute lookups
    n_text_layer = dims.n_text_layer
    n_text_head = dims.n_text_head
    
    self.encoder = AudioEncoder(
        dims.n_mels,
        dims.n_audio_ctx,
        dims.n_audio_state,
        dims.n_audio_head,
        dims.n_audio_layer,
    )
    
    self.decoder = TextDecoder(
        dims.n_vocab,
        dims.n_text_ctx,
        dims.n_text_state,
        n_text_head,
        n_text_layer,
    )
    
    # Optimized alignment heads initialization with direct sparse tensor creation
    # Use the last half of decoder layers for time alignment by default
    half_layer = n_text_layer // 2
    
    # Create sparse COO tensor directly to avoid dense tensor allocation and conversion
    # Only allocate memory for True values (last half of layers)
    indices = torch.stack([
        torch.arange(half_layer, n_text_layer, dtype=torch.long).repeat_interleave(n_text_head),  # layer indices
        torch.arange(n_text_head, dtype=torch.long).repeat(n_text_layer - half_layer)  # head indices
    ])
    values = torch.ones(indices.shape[1], dtype=torch.bool)
    all_heads_sparse = torch.sparse_coo_tensor(
        indices, values, (n_text_layer, n_text_head), dtype=torch.bool
    ).coalesce()  # Coalesce for optimal sparse operations
    
    self.register_buffer(""alignment_heads"", all_heads_sparse, persistent=False)"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"@lru_cache(maxsize=None)

def get_tokenizer(

    multilingual: bool,

    *,

    num_languages: int = 99,

    language: Optional[str] = None,

    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]

) -> Tokenizer:

    if language is not None:

        language = language.lower()

        if language not in LANGUAGES:

            if language in TO_LANGUAGE_CODE:

                language = TO_LANGUAGE_CODE[language]

            else:

                raise ValueError(f""Unsupported language: {language}"")



    if multilingual:

        encoding_name = ""multilingual""

        language = language or ""en""

        task = task or ""transcribe""

    else:

        encoding_name = ""gpt2""

        language = None

        task = None



    encoding = get_encoding(name=encoding_name, num_languages=num_languages)



    return Tokenizer(

        encoding=encoding, num_languages=num_languages, language=language, task=task

    )
"
"@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal[""transcribe"", ""translate"", None]
) -> Tokenizer:
    # Pre-normalize language once if provided, avoiding redundant operations
    if language is not None:
        language_lower = language.lower()
        
        # Use single lookup with get() to avoid double dictionary access
        if language_lower not in LANGUAGES:
            language = TO_LANGUAGE_CODE.get(language_lower)
            if language is None:
                raise ValueError(f""Unsupported language: {language}"")
        else:
            language = language_lower

    # Branch prediction optimization: handle most common case first
    if multilingual:
        encoding_name = ""multilingual""
        language = language or ""en""  # Short-circuit evaluation
        task = task or ""transcribe""
    else:
        encoding_name = ""gpt2""
        language = None
        task = None

    # Cache-friendly: get_encoding call remains unchanged to preserve caching
    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    # Direct construction minimizes object creation overhead
    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"@lru_cache(maxsize=None)

def get_encoding(name: str = ""gpt2"", num_languages: int = 99):

    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")

    ranks = {

        base64.b64decode(token): int(rank)

        for token, rank in (line.split() for line in open(vocab_path) if line)

    }

    n_vocab = len(ranks)

    special_tokens = {}



    specials = [

        ""<|endoftext|>"",

        ""<|startoftranscript|>"",

        *[f""<|{lang}|>"" for lang in list(LANGUAGES.keys())[:num_languages]],

        ""<|translate|>"",

        ""<|transcribe|>"",

        ""<|startoflm|>"",

        ""<|startofprev|>"",

        ""<|nospeech|>"",

        ""<|notimestamps|>"",

        *[f""<|{i * 0.02:.2f}|>"" for i in range(1501)],

    ]



    for token in specials:

        special_tokens[token] = n_vocab

        n_vocab += 1



    return tiktoken.Encoding(

        name=os.path.basename(vocab_path),

        explicit_n_vocab=n_vocab,

        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",

        mergeable_ranks=ranks,

        special_tokens=special_tokens,

    )
"
"@lru_cache(maxsize=None)
def get_encoding(name: str = ""gpt2"", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), ""assets"", f""{name}.tiktoken"")
    
    # Pre-allocate dict with estimated size for better memory efficiency
    ranks = {}
    with open(vocab_path, 'r') as f:
        for line in f:
            if line:
                token, rank = line.split()
                ranks[base64.b64decode(token)] = int(rank)
    
    n_vocab = len(ranks)
    
    # Pre-allocate dict with known size to reduce rehashing
    special_tokens = {}
    
    # Cache language keys slice to avoid repeated list creation
    lang_keys = list(LANGUAGES.keys())[:num_languages]
    
    # Pre-generate timestamp tokens using list comprehension for better performance
    timestamp_tokens = [f""<|{i * 0.02:.2f}|>"" for i in range(1501)]
    
    # Combine all special tokens in single iteration to minimize dict operations
    specials = (
        [""<|endoftext|>"", ""<|startoftranscript|>""] +
        [f""<|{lang}|>"" for lang in lang_keys] +
        [""<|translate|>"", ""<|transcribe|>"", ""<|startoflm|>"", ""<|startofprev|>"", ""<|nospeech|>"", ""<|notimestamps|>""] +
        timestamp_tokens
    )
    
    # Single loop with enumerate for better cache locality and fewer dict lookups
    for i, token in enumerate(specials):
        special_tokens[token] = n_vocab + i
    
    n_vocab += len(specials)
    
    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab,
        pat_str=r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def encode(self, text, **kwargs):

    return self.encoding.encode(text, **kwargs)
"
"def encode(self, text, **kwargs):
    # Direct delegation to underlying encoding - no optimization needed
    # This method is already optimal as it's a simple passthrough
    return self.encoding.encode(text, **kwargs)"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:

    os.makedirs(root, exist_ok=True)



    expected_sha256 = url.split(""/"")[-2]

    download_target = os.path.join(root, os.path.basename(url))



    if os.path.exists(download_target) and not os.path.isfile(download_target):

        raise RuntimeError(f""{download_target} exists and is not a regular file"")



    if os.path.isfile(download_target):

        with open(download_target, ""rb"") as f:

            model_bytes = f.read()

        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:

            return model_bytes if in_memory else download_target

        else:

            warnings.warn(

                f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""

            )



    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:

        with tqdm(

            total=int(source.info().get(""Content-Length"")),

            ncols=80,

            unit=""iB"",

            unit_scale=True,

            unit_divisor=1024,

        ) as loop:

            while True:

                buffer = source.read(8192)

                if not buffer:

                    break



                output.write(buffer)

                loop.update(len(buffer))



    model_bytes = open(download_target, ""rb"").read()

    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:

        raise RuntimeError(

            ""Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.""

        )



    return model_bytes if in_memory else download_target
"
"def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:
    os.makedirs(root, exist_ok=True)
    
    expected_sha256 = url.split(""/"")[-2]
    download_target = os.path.join(root, os.path.basename(url))
    
    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f""{download_target} exists and is not a regular file"")
    
    # Check existing file with streaming hash calculation to avoid loading large files into memory
    if os.path.isfile(download_target):
        sha256_hash = hashlib.sha256()
        try:
            with open(download_target, ""rb"") as f:
                # Use larger buffer for better I/O performance
                for chunk in iter(lambda: f.read(65536), b""""):
                    sha256_hash.update(chunk)
            
            if sha256_hash.hexdigest() == expected_sha256:
                if in_memory:
                    # Only read into memory if needed
                    with open(download_target, ""rb"") as f:
                        return f.read()
                else:
                    return download_target
            else:
                warnings.warn(
                    f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file""
                )
        except IOError:
            # Handle file corruption or access issues
            warnings.warn(f""Error reading {download_target}; re-downloading the file"")
    
    # Download with streaming hash calculation to avoid double I/O
    sha256_hash = hashlib.sha256()
    model_bytes = bytearray() if in_memory else None
    
    with urllib.request.urlopen(url) as source, open(download_target, ""wb"") as output:
        content_length = int(source.info().get(""Content-Length"", 0))
        with tqdm(
            total=content_length,
            ncols=80,
            unit=""iB"",
            unit_scale=True,
            unit_divisor=1024,
        ) as loop:
            # Use larger buffer for better network I/O performance
            while True:
                buffer = source.read(65536)
                if not buffer:
                    break
                
                output.write(buffer)
                sha256_hash.update(buffer)  # Calculate hash during download
                if in_memory:
                    model_bytes.extend(buffer)  # Accumulate bytes if needed
                loop.update(len(buffer))
    
    # Verify hash without additional file read
    if sha256_hash.hexdigest() != expected_sha256:
        raise RuntimeError(
            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )
    
    return bytes(model_bytes) if in_memory else download_target"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
"def median_filter_cuda(x: torch.Tensor, filter_width: int):

    """"""Apply a median filter of given width along the last dimension of x""""""

    slices = x.contiguous().unfold(-1, filter_width, 1)

    grid = np.prod(slices.shape[:-2])



    kernel = median_kernel(filter_width)

    y = torch.empty_like(slices[..., 0])



    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()

    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)



    return y
"
"def median_filter_cuda(x: torch.Tensor, filter_width: int):
    """"""Apply a median filter of given width along the last dimension of x""""""
    
    # Use in-place unfold operation to minimize memory allocation
    slices = x.contiguous().unfold(-1, filter_width, 1)
    
    # Pre-calculate grid size once to avoid repeated computation
    grid_size = slices.numel() // filter_width
    
    # Pre-allocate output tensor with optimal memory layout
    y = torch.empty(slices.shape[:-1], dtype=x.dtype, device=x.device, 
                    memory_format=torch.contiguous_format)
    
    # Get or create cached kernel instance to avoid recompilation
    kernel = median_kernel(filter_width)
    
    # Calculate optimal block size using bitwise operations for efficiency
    # Use max to ensure minimum block size of 32 for warp efficiency
    stride = y.stride(-2) if y.dim() > 1 else 1
    BLOCK_SIZE = max(32, 1 << (stride - 1).bit_length())
    
    # Launch kernel with pre-calculated parameters
    kernel[(grid_size,)](
        y.view(-1),  # Flatten for efficient indexing
        slices.view(-1, filter_width),  # Flatten input slices
        slices.stride(-2), 
        y.stride(-2), 
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return y"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
2
15.5458
"def load_audio(file: str, sr: int = SAMPLE_RATE):

    """"""

    Open an audio file and read as mono waveform, resampling as necessary



    Parameters

    ----------

    file: str

        The audio file to open



    sr: int

        The sample rate to resample the audio if necessary



    Returns

    -------

    A NumPy array containing the audio waveform, in float32 dtype.

    """"""



    # This launches a subprocess to decode audio while down-mixing

    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.

    # fmt: off

    cmd = [

        ""ffmpeg"",

        ""-nostdin"",

        ""-threads"", ""0"",

        ""-i"", file,

        ""-f"", ""s16le"",

        ""-ac"", ""1"",

        ""-acodec"", ""pcm_s16le"",

        ""-ar"", str(sr),

        ""-""

    ]

    # fmt: on

    try:

        out = run(cmd, capture_output=True, check=True).stdout

    except CalledProcessError as e:

        raise RuntimeError(f""Failed to load audio: {e.stderr.decode()}"") from e



    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0
"
"def load_audio(file: str, sr: int = SAMPLE_RATE):
    """"""
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """"""
    
    # Optimized ffmpeg command with performance-focused flags
    cmd = [
        ""ffmpeg"",
        ""-nostdin"",
        ""-threads"", ""0"",  # Use all available CPU cores
        ""-fflags"", ""+discardcorrupt+genpts"",  # Skip corrupted frames for faster processing
        ""-i"", file,
        ""-f"", ""s16le"",
        ""-ac"", ""1"",
        ""-acodec"", ""pcm_s16le"",
        ""-ar"", str(sr),
        ""-map_metadata"", ""-1"",  # Skip metadata parsing
        ""-""
    ]
    
    try:
        # Use Popen for streaming to reduce memory usage and enable parallel processing
        process = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = process.communicate()
        
        if process.returncode != 0:
            raise RuntimeError(f""Failed to load audio: {stderr.decode()}"")
            
    except Exception as e:
        raise RuntimeError(f""Failed to load audio: {str(e)}"") from e
    
    # Direct conversion optimized for cache efficiency and memory alignment
    # Using np.frombuffer with copy=False for zero-copy operation when possible
    audio_int16 = np.frombuffer(stdout, dtype=np.int16, count=-1)
    
    # Vectorized conversion with optimized constant (1/32768 = 3.0517578125e-05)
    # This avoids division and uses multiplication instead
    return audio_int16.astype(np.float32, copy=False) * 3.0517578125e-05"
whisper
40
"Optimize the following code object for best-runtime performance while preserving its signature and interface. Focus on improving algorithmic complexity, enhancing data structure efficiency, minimizing loop iterations, optimizing memory and cache usage, streamlining I/O operations, and leveraging parallel processing. Maintain the clarity and integrity of the code, ensuring all changes are backed by clear, maintainable rationale.

```python
# [Insert the specific code object here]
```

Return only the optimized code, adhering strictly to its original signature and interface, and provide brief in-line comments where necessary to elucidate optimization choices."
MP
0
15.5458
