<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="13" skipped="0" tests="25" time="5.532" timestamp="2025-11-23T16:33:13.274717-08:00" hostname="aiden"><testcase classname="tests.test_audio" name="test_audio" time="0.006"><failure message="FileNotFoundError: [WinError 2] The system cannot find the file specified">def test_audio():
        audio_path = os.path.join(os.path.dirname(__file__), "jfk.flac")
&gt;       audio = load_audio(audio_path)
                ^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_audio.py:10: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
projects\whisper\whisper\audio.py:58: in load_audio
    out = run(cmd, capture_output=True, check=True).stdout
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:548: in run
    with Popen(*popenargs, **kwargs) as process:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1026: in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;Popen: returncode: None args: ['ffmpeg', '-nostdin', '-threads', '0', '-i',...&gt;
args = 'ffmpeg -nostdin -threads 0 -i c:\\Users\\aiden\\OneDrive\\Desktop\\personalprojects\\ahmedlab\\MPCO-dupe\\pipeline\\profiler\\projects\\whisper\\tests\\jfk.flac -f s16le -ac 1 -acodec pcm_s16le -ar 16000 -'
executable = None, preexec_fn = None, close_fds = False, pass_fds = (), cwd = None, env = None, startupinfo = &lt;subprocess.STARTUPINFO object at 0x00000226582489B0&gt;
creationflags = 0, shell = False, p2cread = Handle(880), p2cwrite = -1, c2pread = 14, c2pwrite = Handle(816), errread = 15, errwrite = Handle(1444), unused_restore_signals = True
unused_gid = None, unused_gids = None, unused_uid = None, unused_umask = -1, unused_start_new_session = False, unused_process_group = -1

    def _execute_child(self, args, executable, preexec_fn, close_fds,
                       pass_fds, cwd, env,
                       startupinfo, creationflags, shell,
                       p2cread, p2cwrite,
                       c2pread, c2pwrite,
                       errread, errwrite,
                       unused_restore_signals,
                       unused_gid, unused_gids, unused_uid,
                       unused_umask,
                       unused_start_new_session, unused_process_group):
        """Execute program (MS Windows version)"""
    
        assert not pass_fds, "pass_fds not supported on Windows."
    
        if isinstance(args, str):
            pass
        elif isinstance(args, bytes):
            if shell:
                raise TypeError('bytes args is not allowed on Windows')
            args = list2cmdline([args])
        elif isinstance(args, os.PathLike):
            if shell:
                raise TypeError('path-like args is not allowed when '
                                'shell is true')
            args = list2cmdline([args])
        else:
            args = list2cmdline(args)
    
        if executable is not None:
            executable = os.fsdecode(executable)
    
        # Process startup details
        if startupinfo is None:
            startupinfo = STARTUPINFO()
        else:
            # bpo-34044: Copy STARTUPINFO since it is modified above,
            # so the caller can reuse it multiple times.
            startupinfo = startupinfo.copy()
    
        use_std_handles = -1 not in (p2cread, c2pwrite, errwrite)
        if use_std_handles:
            startupinfo.dwFlags |= _winapi.STARTF_USESTDHANDLES
            startupinfo.hStdInput = p2cread
            startupinfo.hStdOutput = c2pwrite
            startupinfo.hStdError = errwrite
    
        attribute_list = startupinfo.lpAttributeList
        have_handle_list = bool(attribute_list and
                                "handle_list" in attribute_list and
                                attribute_list["handle_list"])
    
        # If we were given an handle_list or need to create one
        if have_handle_list or (use_std_handles and close_fds):
            if attribute_list is None:
                attribute_list = startupinfo.lpAttributeList = {}
            handle_list = attribute_list["handle_list"] = \
                list(attribute_list.get("handle_list", []))
    
            if use_std_handles:
                handle_list += [int(p2cread), int(c2pwrite), int(errwrite)]
    
            handle_list[:] = self._filter_handle_list(handle_list)
    
            if handle_list:
                if not close_fds:
                    warnings.warn("startupinfo.lpAttributeList['handle_list'] "
                                  "overriding close_fds", RuntimeWarning)
    
                # When using the handle_list we always request to inherit
                # handles but the only handles that will be inherited are
                # the ones in the handle_list
                close_fds = False
    
        if shell:
            startupinfo.dwFlags |= _winapi.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = _winapi.SW_HIDE
            if not executable:
                # gh-101283: without a fully-qualified path, before Windows
                # checks the system directories, it first looks in the
                # application directory, and also the current directory if
                # NeedCurrentDirectoryForExePathW(ExeName) is true, so try
                # to avoid executing unqualified "cmd.exe".
                comspec = os.environ.get('ComSpec')
                if not comspec:
                    system_root = os.environ.get('SystemRoot', '')
                    comspec = os.path.join(system_root, 'System32', 'cmd.exe')
                    if not os.path.isabs(comspec):
                        raise FileNotFoundError('shell not found: neither %ComSpec% nor %SystemRoot% is set')
                if os.path.isabs(comspec):
                    executable = comspec
            else:
                comspec = executable
    
            args = '{} /c "{}"'.format (comspec, args)
    
        if cwd is not None:
            cwd = os.fsdecode(cwd)
    
        sys.audit("subprocess.Popen", executable, args, cwd, env)
    
        # Start the process
        try:
&gt;           hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
                                     # no special security
                                     None, None,
                                     int(not close_fds),
                                     creationflags,
                                     env,
                                     cwd,
                                     startupinfo)
E                                    FileNotFoundError: [WinError 2] The system cannot find the file specified

C:\Users\aiden\AppData\Local\Programs\Python\Python312\Lib\subprocess.py:1538: FileNotFoundError</failure></testcase><testcase classname="tests.test_normalizer" name="test_number_normalizer[std0]" time="0.003" /><testcase classname="tests.test_normalizer" name="test_number_normalizer[std1]" time="0.008" /><testcase classname="tests.test_normalizer" name="test_spelling_normalizer" time="0.001" /><testcase classname="tests.test_normalizer" name="test_text_normalizer" time="0.002" /><testcase classname="tests.test_timing" name="test_dtw[10-20]" time="1.700" /><testcase classname="tests.test_timing" name="test_dtw[32-16]" time="0.001" /><testcase classname="tests.test_timing" name="test_dtw[123-1500]" time="0.009" /><testcase classname="tests.test_timing" name="test_dtw[234-189]" time="0.003" /><testcase classname="tests.test_timing" name="test_dtw_cuda_equivalence[10-20]" time="0.001"><failure message="AssertionError: Torch not compiled with CUDA enabled">N = 10, M = 20

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("N, M", sizes)
    def test_dtw_cuda_equivalence(N: int, M: int):
        x_numpy = np.random.randn(N, M).astype(np.float32)
&gt;       x_cuda = torch.from_numpy(x_numpy).cuda()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_timing" name="test_dtw_cuda_equivalence[32-16]" time="0.001"><failure message="AssertionError: Torch not compiled with CUDA enabled">N = 32, M = 16

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("N, M", sizes)
    def test_dtw_cuda_equivalence(N: int, M: int):
        x_numpy = np.random.randn(N, M).astype(np.float32)
&gt;       x_cuda = torch.from_numpy(x_numpy).cuda()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_timing" name="test_dtw_cuda_equivalence[123-1500]" time="0.007"><failure message="AssertionError: Torch not compiled with CUDA enabled">N = 123, M = 1500

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("N, M", sizes)
    def test_dtw_cuda_equivalence(N: int, M: int):
        x_numpy = np.random.randn(N, M).astype(np.float32)
&gt;       x_cuda = torch.from_numpy(x_numpy).cuda()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_timing" name="test_dtw_cuda_equivalence[234-189]" time="0.002"><failure message="AssertionError: Torch not compiled with CUDA enabled">N = 234, M = 189

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("N, M", sizes)
    def test_dtw_cuda_equivalence(N: int, M: int):
        x_numpy = np.random.randn(N, M).astype(np.float32)
&gt;       x_cuda = torch.from_numpy(x_numpy).cuda()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter[shape0]" time="0.001"><failure message="AttributeError: 'NoneType' object has no attribute 'view'">shape = (10,)

    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
&gt;           filtered = median_filter(x, filter_width)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = tensor([[[ 1.2370,  0.6526, -0.2325, -1.4345, -0.6678,  0.6184, -0.1099,
           1.6341,  0.4719,  0.7134]]]), filter_width = 3

    def median_filter(x: torch.Tensor, filter_width: int):
        """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    
        # 1. ALGORITHMIC COMPLEXITY: Early validation with fast modulo check
        # Using bitwise AND for even check (O(1) instead of modulo operation)
        if filter_width &lt;= 0 or (filter_width &amp; 1) == 0:
            raise ValueError("`filter_width` should be an odd positive number")
    
        # 2. MEMORY ACCESS: Cache critical values to reduce repeated calculations
        pad_width = filter_width &gt;&gt; 1  # Bitwise shift for division by 2
        last_dim_size = x.shape[-1]
    
        # 3. ALGORITHMIC COMPLEXITY: Early return optimization
        if last_dim_size &lt;= pad_width:
            return x
    
        # 4. MEMORY ACCESS: Store shape info efficiently
        original_shape = x.shape
        original_ndim = x.ndim
        needs_reshape = original_ndim &lt;= 2
    
        # 5. MEMORY ACCESS: Efficient tensor reshaping using contiguous memory layout
        if needs_reshape:
            # Ensure contiguous memory for better cache performance
            if not x.is_contiguous():
                x = x.contiguous()
            # Use view with minimal memory footprint
            x = x.view(1, 1, -1) if original_ndim == 1 else x.view(1, 1, *original_shape)
    
        # 6. I/O OPERATIONS: Single padding operation with optimal mode
        # Reflect mode provides better boundary handling than other modes
        padded_x = F.pad(x, (pad_width, pad_width, 0, 0), mode="reflect")
    
        # 7. PARALLEL PROCESSING: CUDA optimization with device-specific handling
        result = None
        if padded_x.is_cuda and padded_x.device.type == 'cuda':
            try:
                # Attempt GPU-accelerated median filter
                result = _cuda_median_filter_optimized(padded_x, filter_width, pad_width)
            except (RuntimeError, NotImplementedError):
                # Graceful fallback on CUDA errors
                pass
    
        # 8. REDUNDANT COMPUTATIONS: CPU fallback with memoization potential
        if result is None:
            result = _cpu_median_filter_vectorized(padded_x, filter_width, pad_width, last_dim_size)
    
        # 9. MEMORY ACCESS: Efficient tensor restoration with minimal operations
        if needs_reshape:
            # Single operation to restore original shape
&gt;           result = result.view(original_shape)
                     ^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'view'

projects\whisper\whisper\timing.py:77: AttributeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter[shape1]" time="0.001"><failure message="AttributeError: 'NoneType' object has no attribute 'view'">shape = (1, 15)

    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
&gt;           filtered = median_filter(x, filter_width)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = tensor([[[[-0.4500, -0.6664,  1.3508,  1.7263,  0.1770, -0.2228,  0.1062,
           -1.4887,  0.3592, -0.0862,  0.3232, -0.8627,  0.1280,  0.0836,
           -1.2221]]]])
filter_width = 3

    def median_filter(x: torch.Tensor, filter_width: int):
        """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    
        # 1. ALGORITHMIC COMPLEXITY: Early validation with fast modulo check
        # Using bitwise AND for even check (O(1) instead of modulo operation)
        if filter_width &lt;= 0 or (filter_width &amp; 1) == 0:
            raise ValueError("`filter_width` should be an odd positive number")
    
        # 2. MEMORY ACCESS: Cache critical values to reduce repeated calculations
        pad_width = filter_width &gt;&gt; 1  # Bitwise shift for division by 2
        last_dim_size = x.shape[-1]
    
        # 3. ALGORITHMIC COMPLEXITY: Early return optimization
        if last_dim_size &lt;= pad_width:
            return x
    
        # 4. MEMORY ACCESS: Store shape info efficiently
        original_shape = x.shape
        original_ndim = x.ndim
        needs_reshape = original_ndim &lt;= 2
    
        # 5. MEMORY ACCESS: Efficient tensor reshaping using contiguous memory layout
        if needs_reshape:
            # Ensure contiguous memory for better cache performance
            if not x.is_contiguous():
                x = x.contiguous()
            # Use view with minimal memory footprint
            x = x.view(1, 1, -1) if original_ndim == 1 else x.view(1, 1, *original_shape)
    
        # 6. I/O OPERATIONS: Single padding operation with optimal mode
        # Reflect mode provides better boundary handling than other modes
        padded_x = F.pad(x, (pad_width, pad_width, 0, 0), mode="reflect")
    
        # 7. PARALLEL PROCESSING: CUDA optimization with device-specific handling
        result = None
        if padded_x.is_cuda and padded_x.device.type == 'cuda':
            try:
                # Attempt GPU-accelerated median filter
                result = _cuda_median_filter_optimized(padded_x, filter_width, pad_width)
            except (RuntimeError, NotImplementedError):
                # Graceful fallback on CUDA errors
                pass
    
        # 8. REDUNDANT COMPUTATIONS: CPU fallback with memoization potential
        if result is None:
            result = _cpu_median_filter_vectorized(padded_x, filter_width, pad_width, last_dim_size)
    
        # 9. MEMORY ACCESS: Efficient tensor restoration with minimal operations
        if needs_reshape:
            # Single operation to restore original shape
&gt;           result = result.view(original_shape)
                     ^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'view'

projects\whisper\whisper\timing.py:77: AttributeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter[shape2]" time="0.009"><failure message="TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'">shape = (4, 5, 345)

    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
            filtered = median_filter(x, filter_width)
    
            # using np.pad to reflect-pad, because Scipy's behavior is different near the edges.
            pad_width = filter_width // 2
            padded_x = np.pad(
                x, [(0, 0)] * (x.ndim - 1) + [(pad_width, pad_width)], mode="reflect"
            )
            scipy_filtered = scipy.ndimage.median_filter(
                padded_x, [1] * (x.ndim - 1) + [filter_width]
            )
            scipy_filtered = scipy_filtered[..., pad_width:-pad_width]
    
&gt;           assert np.allclose(filtered, scipy_filtered)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = None
b = array([[[ 0.8483273 ,  0.7652827 ,  0.8483273 , ..., -0.07364003,
         -0.07364003, -0.9308422 ],
        [ 1.1353...15, -0.02251115, -0.63057137, ...,  1.4818304 ,
          0.13470417,  1.4818304 ]]], shape=(4, 5, 345), dtype=float32)
rtol = 1e-05, atol = 1e-08, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     with magnitudes much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : array_like
            The relative tolerance parameter (see Notes).
        atol : array_like
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.::
    
         absolute(a - b) &lt;= (atol + rtol * absolute(b))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`.
    
        The default value of `atol` is not appropriate when the reference value
        `b` has magnitude smaller than one. For example, it is unlikely that
        ``a = 1e-9`` and ``b = 2e-9`` should be considered "close", yet
        ``isclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure
        to select `atol` for the use case at hand, especially for defining the
        threshold below which a non-zero value in `a` will be considered "close"
        to a very small or zero value in `b`.
    
        `isclose` is not defined for non-numeric data types.
        :class:`bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
    
        &gt;&gt;&gt; np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
    
        &gt;&gt;&gt; np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
    
        &gt;&gt;&gt; np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
    
        &gt;&gt;&gt; np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
    
        &gt;&gt;&gt; np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
    
        """
        # Turn all but python scalars into arrays.
        x, y, atol, rtol = (
            a if isinstance(a, (int, float, complex)) else asanyarray(a)
            for a in (a, b, atol, rtol))
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if (dtype := getattr(y, "dtype", None)) is not None and dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
        elif isinstance(y, int):
            y = float(y)
    
        # atol and rtol can be arrays
        if not (np.all(np.isfinite(atol)) and np.all(np.isfinite(rtol))):
            err_s = np.geterr()["invalid"]
            err_msg = f"One of rtol or atol is not valid, atol: {atol}, rtol: {rtol}"
    
            if err_s == "warn":
                warnings.warn(err_msg, RuntimeWarning, stacklevel=2)
            elif err_s == "raise":
                raise FloatingPointError(err_msg)
            elif err_s == "print":
                print(err_msg)
    
        with errstate(invalid='ignore'):
    
&gt;           result = (less_equal(abs(x - y), atol + rtol * abs(y))
                                     ^^^^^
                      &amp; isfinite(y)
                      | (x == y))
E           TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'

venv\Lib\site-packages\numpy\_core\numeric.py:2496: TypeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter[shape3]" time="0.295"><failure message="TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'">shape = (6, 12, 240, 512)

    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
            filtered = median_filter(x, filter_width)
    
            # using np.pad to reflect-pad, because Scipy's behavior is different near the edges.
            pad_width = filter_width // 2
            padded_x = np.pad(
                x, [(0, 0)] * (x.ndim - 1) + [(pad_width, pad_width)], mode="reflect"
            )
            scipy_filtered = scipy.ndimage.median_filter(
                padded_x, [1] * (x.ndim - 1) + [filter_width]
            )
            scipy_filtered = scipy_filtered[..., pad_width:-pad_width]
    
&gt;           assert np.allclose(filtered, scipy_filtered)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv\Lib\site-packages\numpy\_core\numeric.py:2365: in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = None
b = array([[[[ 1.39143920e+00,  4.73593146e-01, -1.02997088e+00, ...,
           1.64780170e-02,  9.93736565e-01,  9.93736...01, ...,
           3.93249065e-01, -3.45369726e-01, -2.05213785e+00]]]],
      shape=(6, 12, 240, 512), dtype=float32)
rtol = 1e-05, atol = 1e-08, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     with magnitudes much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : array_like
            The relative tolerance parameter (see Notes).
        atol : array_like
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.::
    
         absolute(a - b) &lt;= (atol + rtol * absolute(b))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`.
    
        The default value of `atol` is not appropriate when the reference value
        `b` has magnitude smaller than one. For example, it is unlikely that
        ``a = 1e-9`` and ``b = 2e-9`` should be considered "close", yet
        ``isclose(1e-9, 2e-9)`` is ``True`` with default settings. Be sure
        to select `atol` for the use case at hand, especially for defining the
        threshold below which a non-zero value in `a` will be considered "close"
        to a very small or zero value in `b`.
    
        `isclose` is not defined for non-numeric data types.
        :class:`bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
    
        &gt;&gt;&gt; np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
    
        &gt;&gt;&gt; np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
    
        &gt;&gt;&gt; np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
    
        &gt;&gt;&gt; np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
    
        &gt;&gt;&gt; np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
    
        &gt;&gt;&gt; np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
    
        """
        # Turn all but python scalars into arrays.
        x, y, atol, rtol = (
            a if isinstance(a, (int, float, complex)) else asanyarray(a)
            for a in (a, b, atol, rtol))
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if (dtype := getattr(y, "dtype", None)) is not None and dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
        elif isinstance(y, int):
            y = float(y)
    
        # atol and rtol can be arrays
        if not (np.all(np.isfinite(atol)) and np.all(np.isfinite(rtol))):
            err_s = np.geterr()["invalid"]
            err_msg = f"One of rtol or atol is not valid, atol: {atol}, rtol: {rtol}"
    
            if err_s == "warn":
                warnings.warn(err_msg, RuntimeWarning, stacklevel=2)
            elif err_s == "raise":
                raise FloatingPointError(err_msg)
            elif err_s == "print":
                print(err_msg)
    
        with errstate(invalid='ignore'):
    
&gt;           result = (less_equal(abs(x - y), atol + rtol * abs(y))
                                     ^^^^^
                      &amp; isfinite(y)
                      | (x == y))
E           TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'

venv\Lib\site-packages\numpy\_core\numeric.py:2496: TypeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter_equivalence[shape0]" time="0.001"><failure message="AttributeError: 'NoneType' object has no attribute 'view'">shape = (10,)

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter_equivalence(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
&gt;           filtered_cpu = median_filter(x, filter_width)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = tensor([[[ 0.3413, -0.5254,  0.6596, -0.3763, -0.3372, -1.8516, -0.8344,
          -0.0340,  2.3752,  1.3100]]]), filter_width = 3

    def median_filter(x: torch.Tensor, filter_width: int):
        """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    
        # 1. ALGORITHMIC COMPLEXITY: Early validation with fast modulo check
        # Using bitwise AND for even check (O(1) instead of modulo operation)
        if filter_width &lt;= 0 or (filter_width &amp; 1) == 0:
            raise ValueError("`filter_width` should be an odd positive number")
    
        # 2. MEMORY ACCESS: Cache critical values to reduce repeated calculations
        pad_width = filter_width &gt;&gt; 1  # Bitwise shift for division by 2
        last_dim_size = x.shape[-1]
    
        # 3. ALGORITHMIC COMPLEXITY: Early return optimization
        if last_dim_size &lt;= pad_width:
            return x
    
        # 4. MEMORY ACCESS: Store shape info efficiently
        original_shape = x.shape
        original_ndim = x.ndim
        needs_reshape = original_ndim &lt;= 2
    
        # 5. MEMORY ACCESS: Efficient tensor reshaping using contiguous memory layout
        if needs_reshape:
            # Ensure contiguous memory for better cache performance
            if not x.is_contiguous():
                x = x.contiguous()
            # Use view with minimal memory footprint
            x = x.view(1, 1, -1) if original_ndim == 1 else x.view(1, 1, *original_shape)
    
        # 6. I/O OPERATIONS: Single padding operation with optimal mode
        # Reflect mode provides better boundary handling than other modes
        padded_x = F.pad(x, (pad_width, pad_width, 0, 0), mode="reflect")
    
        # 7. PARALLEL PROCESSING: CUDA optimization with device-specific handling
        result = None
        if padded_x.is_cuda and padded_x.device.type == 'cuda':
            try:
                # Attempt GPU-accelerated median filter
                result = _cuda_median_filter_optimized(padded_x, filter_width, pad_width)
            except (RuntimeError, NotImplementedError):
                # Graceful fallback on CUDA errors
                pass
    
        # 8. REDUNDANT COMPUTATIONS: CPU fallback with memoization potential
        if result is None:
            result = _cpu_median_filter_vectorized(padded_x, filter_width, pad_width, last_dim_size)
    
        # 9. MEMORY ACCESS: Efficient tensor restoration with minimal operations
        if needs_reshape:
            # Single operation to restore original shape
&gt;           result = result.view(original_shape)
                     ^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'view'

projects\whisper\whisper\timing.py:77: AttributeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter_equivalence[shape1]" time="0.001"><failure message="AttributeError: 'NoneType' object has no attribute 'view'">shape = (1, 15)

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter_equivalence(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
&gt;           filtered_cpu = median_filter(x, filter_width)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

projects\whisper\tests\test_timing.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = tensor([[[[-0.7732,  1.0150,  0.2160,  1.4779,  0.0136, -0.9894, -1.0702,
            0.9297,  1.2045,  0.3116, -0.8250,  0.9133, -0.1693, -0.5137,
            0.5182]]]])
filter_width = 3

    def median_filter(x: torch.Tensor, filter_width: int):
        """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    
        # 1. ALGORITHMIC COMPLEXITY: Early validation with fast modulo check
        # Using bitwise AND for even check (O(1) instead of modulo operation)
        if filter_width &lt;= 0 or (filter_width &amp; 1) == 0:
            raise ValueError("`filter_width` should be an odd positive number")
    
        # 2. MEMORY ACCESS: Cache critical values to reduce repeated calculations
        pad_width = filter_width &gt;&gt; 1  # Bitwise shift for division by 2
        last_dim_size = x.shape[-1]
    
        # 3. ALGORITHMIC COMPLEXITY: Early return optimization
        if last_dim_size &lt;= pad_width:
            return x
    
        # 4. MEMORY ACCESS: Store shape info efficiently
        original_shape = x.shape
        original_ndim = x.ndim
        needs_reshape = original_ndim &lt;= 2
    
        # 5. MEMORY ACCESS: Efficient tensor reshaping using contiguous memory layout
        if needs_reshape:
            # Ensure contiguous memory for better cache performance
            if not x.is_contiguous():
                x = x.contiguous()
            # Use view with minimal memory footprint
            x = x.view(1, 1, -1) if original_ndim == 1 else x.view(1, 1, *original_shape)
    
        # 6. I/O OPERATIONS: Single padding operation with optimal mode
        # Reflect mode provides better boundary handling than other modes
        padded_x = F.pad(x, (pad_width, pad_width, 0, 0), mode="reflect")
    
        # 7. PARALLEL PROCESSING: CUDA optimization with device-specific handling
        result = None
        if padded_x.is_cuda and padded_x.device.type == 'cuda':
            try:
                # Attempt GPU-accelerated median filter
                result = _cuda_median_filter_optimized(padded_x, filter_width, pad_width)
            except (RuntimeError, NotImplementedError):
                # Graceful fallback on CUDA errors
                pass
    
        # 8. REDUNDANT COMPUTATIONS: CPU fallback with memoization potential
        if result is None:
            result = _cpu_median_filter_vectorized(padded_x, filter_width, pad_width, last_dim_size)
    
        # 9. MEMORY ACCESS: Efficient tensor restoration with minimal operations
        if needs_reshape:
            # Single operation to restore original shape
&gt;           result = result.view(original_shape)
                     ^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'view'

projects\whisper\whisper\timing.py:77: AttributeError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter_equivalence[shape2]" time="0.001"><failure message="AssertionError: Torch not compiled with CUDA enabled">shape = (4, 5, 345)

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter_equivalence(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
            filtered_cpu = median_filter(x, filter_width)
&gt;           filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                         ^^^^^^^^

projects\whisper\tests\test_timing.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_timing" name="test_median_filter_equivalence[shape3]" time="0.057"><failure message="AssertionError: Torch not compiled with CUDA enabled">shape = (6, 12, 240, 512)

    @pytest.mark.requires_cuda
    @pytest.mark.parametrize("shape", shapes)
    def test_median_filter_equivalence(shape):
        x = torch.randn(*shape)
    
        for filter_width in [3, 5, 7, 13]:
            filtered_cpu = median_filter(x, filter_width)
&gt;           filtered_gpu = median_filter(x.cuda(), filter_width).cpu()
                                         ^^^^^^^^

projects\whisper\tests\test_timing.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, "is_initializing"):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method"
                )
            if not hasattr(torch._C, "_cuda_getDeviceCount"):
&gt;               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

venv\Lib\site-packages\torch\cuda\__init__.py:403: AssertionError</failure></testcase><testcase classname="tests.test_tokenizer" name="test_tokenizer[True]" time="0.003" /><testcase classname="tests.test_tokenizer" name="test_tokenizer[False]" time="0.001" /><testcase classname="tests.test_tokenizer" name="test_multilingual_tokenizer" time="0.122" /><testcase classname="tests.test_tokenizer" name="test_split_on_unicode" time="0.001" /></testsuite></testsuites>